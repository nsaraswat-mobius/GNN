name: Load Weights from CDN
description: Downloads model weights from a CDN URL with retry logic and makes them available for inference.
inputs:
  - name: weights_cdn_url
    type: string
    description: "CDN URL to download model weights (.pth/.pt/.pkl)"
  - name: access_token
    type: string
    description: "Optional Bearer access token file for CDN auth"
  - name: pickle_cdn_url
    type: string
    description: "CDN URL to download supplementary pickle file"
outputs:
  - name: model_weights
    type: Model
    description: "Downloaded model weights for inference"
  - name: pickle_file
    type: Dataset
    description: "Downloaded pickle file"
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v12
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, requests, logging, base64, re
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from urllib.parse import unquote
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--weights_cdn_url', type=str, required=True, help='CDN URL of weights file')
        parser.add_argument('--access_token', type=str, required=False, help='Path to access token file')
        parser.add_argument('--pickle_cdn_url', type=str, required=False, help='CDN URL of pickle file')
        parser.add_argument('--model_weights', type=str, required=True, help='Path to save model weights')
        parser.add_argument('--pickle_file', type=str, required=True, help='Path to save pickle file')
        args = parser.parse_args()
        
        # Load token if provided
        headers = {}
        if args.access_token and os.path.exists(args.access_token):
            with open(args.access_token, 'r') as f:
                token = f.read().strip()
                headers["Authorization"] = f"Bearer {token}"
        
        # Setup retry logger
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("cdn_retry")
        
        # Setup session with retries
        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=2,  # Increased backoff
            status_forcelist=[400, 500, 502, 503, 504],  # Added 400 for temp failures
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        def decode_cdn_url(url):
            # Handle both standard URL encoding and custom _ENC() wrapper
            decoded_url = unquote(url)
            if '_ENC(' in decoded_url:
                logger.warning(f"URL contains _ENC() wrapper: {decoded_url}")
                return decoded_url
            return decoded_url
        
        def download_file(url, out_path, name):
            try:
                processed_url = decode_cdn_url(url)
                logger.info(f"Downloading {name} from {processed_url} with retry + timeout")
                logger.info(f"Original URL: {url}")
                logger.info(f"Processed URL: {processed_url}")
                logger.info(f"Headers: {dict(headers)}")
                resp = session.get(processed_url, headers=headers, stream=True, timeout=120)
                resp.raise_for_status()
                os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
                total_size = int(resp.headers.get('content-length', 0))
                downloaded = 0
                with open(out_path, "wb") as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            if total_size > 0:
                                percent = (downloaded / total_size) * 100
                                if downloaded % (1024 * 1024) == 0:
                                    logger.info(f"Downloaded {downloaded}/{total_size} bytes ({percent:.1f}%)")
                
                logger.info(f"{name} successfully downloaded to {out_path} ({downloaded} bytes)")
                return True
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 400:
                    logger.error(f"400 Bad Request - URL may be malformed or expired: {processed_url}")
                    logger.error(f"Response content: {e.response.text[:500]}")
                raise
            except Exception as e:
                logger.error(f"Download failed for {name}: {str(e)}")
                raise
        
        os.makedirs(os.path.dirname(args.model_weights) or ".", exist_ok=True)
        if args.pickle_cdn_url:
            os.makedirs(os.path.dirname(args.pickle_file) or ".", exist_ok=True)
        try:
            download_file(args.weights_cdn_url, args.model_weights, "weights")
            
            if args.pickle_cdn_url:
                download_file(args.pickle_cdn_url, args.pickle_file, "pickle file")
            else:
                with open(args.pickle_file, 'wb') as f:
                    pass
                logger.info(f"Created empty pickle file at {args.pickle_file}")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to download file after retries: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            raise
    args:
      - --weights_cdn_url
      - inputValue: weights_cdn_url
      - --access_token
      - inputPath: access_token
      - --pickle_cdn_url
      - inputValue: pickle_cdn_url
      - --model_weights
      - outputPath: model_weights
      - --pickle_file
      - outputPath: pickle_file
