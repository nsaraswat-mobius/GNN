name: Load JSON dataset with anomaly detection
description: Fetches JSON from API and prepares train/test/anomaly datasets with float-only features.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: numeric_cols, type: String, description: 'JSON-style string list of numeric columns'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}
  - {name: num_numeric_cols, type: Integer}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn || \
        python3 -m pip install --quiet requests pandas scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import json
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--numeric_cols', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--anomaly_data', type=str, required=True)
        parser.add_argument('--num_numeric_cols', type=str, required=True)
        args = parser.parse_args()

        # Read token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Fetch dataset
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {access_token}"}
        payload = {
            "dbType": "TIDB", "entityId": "", "entityIds": [],
            "ownedOnly": False, "projections": [], "filter": {},
            "startTime": 0, "endTime": 0
        }
        resp = requests.post(args.api_url, headers=headers, json=payload)
        resp.raise_for_status()
        raw_data = resp.json()
        df = pd.DataFrame(raw_data)

        # Parse numeric_cols string into Python list
        numeric_cols = json.loads(args.numeric_cols)

        # Convert everything numeric where possible
        df = df.apply(pd.to_numeric, errors="coerce")

        # Keep only numeric columns
        df = df.select_dtypes(include=[np.number])

        # Keep only requested columns that actually exist
        present_cols = [col for col in numeric_cols if col in df.columns]
        df = df[present_cols]

        # Handle anomalies safely
        if "reachable_frac" in df.columns:
            anomaly_labels = (df["reachable_frac"] == 0.0).astype(np.int32)
        else:
            print("Warning: 'reachable_frac' column not found. Using all zeros for anomaly labels.")
            anomaly_labels = pd.Series(np.zeros(len(df), dtype=np.int32), index=df.index)

        # Train/test split
        if anomaly_labels.nunique() > 1:  # stratify only if more than 1 class
            train_df, test_df, train_labels, test_labels = train_test_split(
                df, anomaly_labels, test_size=0.2, random_state=42, stratify=anomaly_labels
            )
        else:
            train_df, test_df, train_labels, test_labels = train_test_split(
                df, anomaly_labels, test_size=0.2, random_state=42
            )

        # Save outputs
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        with open(args.train_data, "wb") as f:
            pickle.dump(train_df, f)

        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        with open(args.test_data, "wb") as f:
            pickle.dump(test_df, f)

        os.makedirs(os.path.dirname(args.anomaly_data) or ".", exist_ok=True)
        with open(args.anomaly_data, "wb") as f:
            pickle.dump(test_labels, f)

        # Save number of numeric columns actually used
        num_cols = len(present_cols)
        os.makedirs(os.path.dirname(args.num_numeric_cols) or ".", exist_ok=True)
        with open(args.num_numeric_cols, "w") as f:
            f.write(str(num_cols))

        print(f"Successfully processed and saved dataset. Using {num_cols} numeric columns.")

    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --numeric_cols
      - '["endpoint_count", "reachable_frac", "deg_cent", "betw_cent", "clustering", "pagerank"]'
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --anomaly_data
      - {outputPath: anomaly_data}
      - --num_numeric_cols
      - {outputPath: num_numeric_cols}
