apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: stgnn-data-loading-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
    pipelines.kubeflow.org/pipeline_compilation_time: '2024-01-01T00:00:00'
    pipelines.kubeflow.org/pipeline_spec: |
      {
        "description": "STGNN Data Loading Pipeline - Brick 1",
        "name": "STGNN Data Loading"
      }
  labels:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
spec:
  entrypoint: stgnn-data-loading
  templates:
  - name: stgnn-data-loading
    dag:
      tasks:
      - name: data-loading-task
        template: data-loading-component
        arguments:
          parameters:
          - name: input-path
            value: "{{workflow.parameters.input-path}}"
          - name: encoding
            value: "{{workflow.parameters.encoding}}"
          - name: required-fields
            value: "{{workflow.parameters.required-fields}}"
          - name: validate-sample-size
            value: "{{workflow.parameters.validate-sample-size}}"
          - name: sample-size
            value: "{{workflow.parameters.sample-size}}"
          - name: check-duplicates
            value: "{{workflow.parameters.check-duplicates}}"

  - name: data-loading-component
    container:
      image: python:3.9-slim
      command: [python, -c]
      args:
      - |
        import json
        import pandas as pd
        import logging
        from datetime import datetime
        from typing import List, Dict, Any, Optional
        import os
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        class DataLoader:
            """STGNN Data Loading Component for Kubeflow"""
            
            def __init__(self):
                self.logs = []
                self.metadata = {}
                self.validation_results = {}
            
            def load_from_file(self, file_path: str) -> Dict[str, Any]:
                """Load JSON logs from file"""
                try:
                    logger.info(f"Loading data from: {file_path}")
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.logs = json.load(f)
                    
                    # Validate data
                    self.validation_results = self._validate_data()
                    
                    # Create metadata
                    self.metadata = {
                        'file_path': file_path,
                        'total_logs': len(self.logs),
                        'load_timestamp': datetime.now().isoformat(),
                        'validation_passed': self.validation_results['is_valid'],
                        'validation_errors': self.validation_results['errors']
                    }
                    
                    logger.info(f"✅ Successfully loaded {len(self.logs)} logs")
                    logger.info(f"✅ Validation: {'PASSED' if self.validation_results['is_valid'] else 'FAILED'}")
                    
                    return {
                        'logs': self.logs,
                        'metadata': self.metadata,
                        'validation': self.validation_results
                    }
                    
                except Exception as e:
                    logger.error(f"❌ Error loading data: {e}")
                    raise
            
            def _validate_data(self) -> Dict[str, Any]:
                """Validate the loaded data"""
                errors = []
                warnings = []
                required_fields = ["timestamp", "value", "serviceName"]
                
                if not self.logs:
                    errors.append("No logs found in data")
                    return {
                        'is_valid': False,
                        'errors': errors,
                        'warnings': warnings
                    }
                
                # Check if logs is a list
                if not isinstance(self.logs, list):
                    errors.append("Data must be a list of log entries")
                    return {
                        'is_valid': False,
                        'errors': errors,
                        'warnings': warnings
                    }
                
                # Check required fields in first few logs
                sample_size = min(10, len(self.logs))
                
                for i, log in enumerate(self.logs[:sample_size]):
                    if not isinstance(log, dict):
                        errors.append(f"Log {i} is not a dictionary")
                        continue
                    
                    for field in required_fields:
                        if field not in log:
                            errors.append(f"Log {i} missing required field: {field}")
                
                # Check data types and values
                for i, log in enumerate(self.logs[:sample_size]):
                    # Check timestamp
                    if 'timestamp' in log:
                        try:
                            float(log['timestamp'])
                        except (ValueError, TypeError):
                            errors.append(f"Log {i} has invalid timestamp: {log['timestamp']}")
                    
                    # Check value
                    if 'value' in log:
                        try:
                            float(log['value'])
                        except (ValueError, TypeError):
                            warnings.append(f"Log {i} has non-numeric value: {log['value']}")
                
                return {
                    'is_valid': len(errors) == 0,
                    'errors': errors,
                    'warnings': warnings,
                    'total_logs': len(self.logs),
                    'sample_size_checked': sample_size
                }
            
            def get_sample(self, n: int = 5) -> List[Dict[str, Any]]:
                """Get a sample of logs for inspection"""
                return self.logs[:n]
            
            def get_summary(self) -> Dict[str, Any]:
                """Get summary statistics of loaded data"""
                if not self.logs:
                    return {"error": "No data loaded"}
                
                # Count by service
                service_counts = {}
                for log in self.logs:
                    service = log.get('serviceName', 'Unknown')
                    service_counts[service] = service_counts.get(service, 0) + 1
                
                # Get timestamp range
                timestamps = []
                for log in self.logs:
                    try:
                        timestamps.append(float(log.get('timestamp', 0)))
                    except (ValueError, TypeError):
                        pass
                
                timestamp_range = {
                    'min': min(timestamps) if timestamps else None,
                    'max': max(timestamps) if timestamps else None,
                    'span': max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0
                }
                
                return {
                    'total_logs': len(self.logs),
                    'service_distribution': service_counts,
                    'timestamp_range': timestamp_range,
                    'validation_status': self.validation_results['is_valid'],
                    'validation_errors': len(self.validation_results['errors']),
                    'validation_warnings': len(self.validation_results['warnings'])
                }
        
        # Main execution
        def main():
            # Get parameters from environment variables
            input_path = os.environ.get('INPUT_PATH', '/tmp/data/response.json')
            encoding = os.environ.get('ENCODING', 'utf-8')
            sample_size = int(os.environ.get('SAMPLE_SIZE', '5'))
            
            # Initialize loader
            loader = DataLoader()
            
            # Load data
            result = loader.load_from_file(input_path)
            
            # Get sample and summary
            sample = loader.get_sample(sample_size)
            summary = loader.get_summary()
            
            # Save outputs
            os.makedirs('/tmp/outputs', exist_ok=True)
            
            # Save logs
            with open('/tmp/outputs/logs.json', 'w') as f:
                json.dump(result['logs'], f, indent=2)
            
            # Save metadata
            with open('/tmp/outputs/metadata.json', 'w') as f:
                json.dump(result['metadata'], f, indent=2)
            
            # Save validation results
            with open('/tmp/outputs/validation.json', 'w') as f:
                json.dump(result['validation'], f, indent=2)
            
            # Save sample
            with open('/tmp/outputs/sample.json', 'w') as f:
                json.dump(sample, f, indent=2)
            
            # Save summary
            with open('/tmp/outputs/summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            # Print completion message
            print(f"✅ Data loading completed successfully!")
            print(f"✅ Loaded {result['metadata']['total_logs']} logs")
            print(f"✅ Validation: {'PASSED' if result['validation']['is_valid'] else 'FAILED'}")
            print(f"✅ Outputs saved to /tmp/outputs/")
            
            return 0
        
        if __name__ == "__main__":
            exit(main())
      env:
      - name: INPUT_PATH
        value: "{{inputs.parameters.input-path}}"
      - name: ENCODING
        value: "{{inputs.parameters.encoding}}"
      - name: REQUIRED_FIELDS
        value: "{{inputs.parameters.required-fields}}"
      - name: VALIDATE_SAMPLE_SIZE
        value: "{{inputs.parameters.validate-sample-size}}"
      - name: SAMPLE_SIZE
        value: "{{inputs.parameters.sample-size}}"
      - name: CHECK_DUPLICATES
        value: "{{inputs.parameters.check-duplicates}}"
      volumeMounts:
      - name: data-volume
        mountPath: /tmp/data
      - name: output-volume
        mountPath: /tmp/outputs
    inputs:
      parameters:
      - name: input-path
        value: "/tmp/data/response.json"
      - name: encoding
        value: "utf-8"
      - name: required-fields
        value: '["timestamp", "value", "serviceName"]'
      - name: validate-sample-size
        value: "10"
      - name: sample-size
        value: "5"
      - name: check-duplicates
        value: "true"
    outputs:
      artifacts:
      - name: logs
        path: /tmp/outputs/logs.json
      - name: metadata
        path: /tmp/outputs/metadata.json
      - name: validation
        path: /tmp/outputs/validation.json
      - name: sample
        path: /tmp/outputs/sample.json
      - name: summary
        path: /tmp/outputs/summary.json

  arguments:
    parameters:
    - name: input-path
      value: "/tmp/data/response.json"
    - name: encoding
      value: "utf-8"
    - name: required-fields
      value: '["timestamp", "value", "serviceName"]'
    - name: validate-sample-size
      value: "10"
    - name: sample-size
      value: "5"
    - name: check-duplicates
      value: "true"

  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: data-pvc
  - name: output-volume
    persistentVolumeClaim:
      claimName: output-pvc
