name: V_Train_GNN_FF_Fixed
description: Trains STGNN with Backprop / CAFO / Forward-Forward
inputs:
  - {name: data_path, type: Dataset}
  - {name: model, type: Model}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, sys, json, pickle, argparse
        import torch
        import numpy as np
        import torch.nn.functional as F

        # =====================================================
        # BLOCK TORCHVISION (circular import fix)
        # =====================================================
        original_import = __builtins__.__import__
        def block_torchvision(name, *args, **kwargs):
            if "torchvision" in name:
                import types
                dummy = types.ModuleType(name)
                sys.modules[name] = dummy
                return dummy
            return original_import(name, *args, **kwargs)
        __builtins__.__import__ = block_torchvision

        from nesy_factory.GNNs import create_model
        import nesy_factory.GNNs.stgnn as stgnn_module

        __builtins__.__import__ = original_import

        # MONKEY PATCH LayerNorm
        original_layernorm_init = stgnn_module.LayerNorm.__init__
        def patched_layernorm_init(self, normalized_shape, eps=1e-05, elementwise_affine=True):
            if isinstance(normalized_shape, (list, tuple)):
                fixed_shape = tuple(d for d in normalized_shape if d > 0)
                if not fixed_shape or len(fixed_shape) > 1:
                    default_shape = normalized_shape[0] if normalized_shape and normalized_shape[0] > 0 else 16
                    fixed_shape = (default_shape,)
                normalized_shape = fixed_shape
            original_layernorm_init(self, normalized_shape, eps, elementwise_affine)

        stgnn_module.LayerNorm.__init__ = patched_layernorm_init
        STGNN = stgnn_module.STGNN

        # HELPER CLASSES
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std

        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys

            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        yield (self.xs[start_ind: end_ind, ...], self.ys[start_ind: end_ind, ...])
                        self.current_ind += 1
                return _wrapper()

        class STGNNDataWrapper:
            def __init__(self, x_data, y_data, batch_size=32):
                self.x_data = x_data
                self.y_data = y_data
                self.scaler = StandardScaler(x_data.mean(), x_data.std())
                self.train_loader = DataLoaderM(x_data.numpy(), y_data.numpy(), batch_size)

        parser = argparse.ArgumentParser()
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--epoch_loss', type=str, required=True)
        args = parser.parse_args()

        # Load data
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)
        
        X = data.x if hasattr(data, "x") else data.x_train
        Y = data.y if hasattr(data, "y") else data.y_train
        X = torch.tensor(X, dtype=torch.float32) if not isinstance(X, torch.Tensor) else X
        Y = torch.tensor(Y, dtype=torch.float32) if not isinstance(Y, torch.Tensor) else Y

        config = json.loads(args.config) if args.config.startswith('{') else json.load(open(args.config))

        # =================================================================
        # RECEPTIVE FIELD CHECK & PADDING 
        # =================================================================
        layers = config.get('layers', 2)
        dilation_exponential = config.get('dilation_exponential', 2)
        kernel_size = config.get('kernel_size', 2)

        if dilation_exponential > 1 and kernel_size > 1:
            R = 1 + (kernel_size - 1) * ((dilation_exponential**layers - 1) // (dilation_exponential - 1))
        else:
            R = layers * (kernel_size - 1) + 1
        
        min_seq_len = R + config.get('seq_out_len', 1) 
        current_seq_len = X.shape[-1] 

        if current_seq_len < min_seq_len:
            padding_needed = min_seq_len - current_seq_len
            X = F.pad(X, (0, padding_needed)) 
            print(f"Padded X to {X.shape}")

        config['seq_in_len'] = X.shape[-1]
        model = STGNN(config)
        
        # Training Logic
        training_mode = 'cafo' if config.get('use_cafo') else ('forward_forward' if config.get('use_forward_forward') else 'backprop')
        epoch_loss_data = []

        try:
            if training_mode in ['cafo', 'forward_forward']:
                st_data = STGNNDataWrapper(X, Y, batch_size=config.get('batch_size', 32))
                res = model.train_cafo(st_data) if training_mode == 'cafo' else model.train_forward_forward(st_data)
                # Parse result for losses...
            else:
                optimizer = torch.optim.Adam(model.parameters(), lr=config.get('learning_rate', 0.001))
                for epoch in range(config.get('epochs', 10)):
                    model.train()
                    X_input = X.unsqueeze(1) if len(X.shape) == 3 else X
                    optimizer.zero_grad()
                    output = model(X_input)
                    Y_target = Y[..., :output.shape[-1]]
                    loss = F.mse_loss(output, Y_target)
                    loss.backward()
                    optimizer.step()
                    epoch_loss_data.append({'epoch': epoch+1, 'loss': float(loss.item())})

            # Save Results
            os.makedirs(os.path.dirname(args.epoch_loss), exist_ok=True)
            with open(args.epoch_loss, 'w') as f:
                json.dump({'epoch_losses': epoch_loss_data}, f)
            
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            torch.save(model.state_dict(), args.trained_model)
        except Exception as e:
            print(f"Error: {e}")
            raise

    args:
      - --data_path
      - {inputPath: data_path}
      - --model
      - {inputPath: model}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
