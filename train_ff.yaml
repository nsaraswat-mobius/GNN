name: V_Train_GNN_FF_Fixed_Full
description: Robust STGNN Training - All Classes Preserved - Dimension Fix Included
inputs:
  - {name: data_path, type: Dataset, description: 'Path to the input pickle data'}
  - {name: model, type: Model, description: 'Base model input'}
  - {name: config, type: String, description: 'Model configuration string'}
outputs:
  - {name: trained_model, type: Model, description: 'Saved state dict of the trained model'}
  - {name: epoch_loss, type: String, description: 'JSON string containing loss history'}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, sys, json, pickle, argparse, torch
        import numpy as np
        import torch.nn.functional as F

        # =====================================================
        # 1. CORE DATA CLASSES (Required for Pickle Loading)
        # =====================================================
        class DataWrapper:
            """Preserves the structure of the incoming pickle data."""
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean

        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys
            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        yield (self.xs[start_ind: end_ind, ...], self.ys[start_ind: end_ind, ...])
                        self.current_ind += 1
                return _wrapper()

        class STGNNDataWrapper:
            def __init__(self, x_data, y_data, batch_size=32):
                self.x_data = x_data
                self.y_data = y_data
                self.scaler = StandardScaler(x_data.mean().item(), x_data.std().item())
                self.train_loader = DataLoaderM(x_data.numpy(), y_data.numpy(), batch_size)

        # =====================================================
        # 2. RUNTIME STGNN PATCHING (FOR V38-GPU STABILITY)
        # =====================================================
        try:
            import nesy_factory.GNNs.stgnn as stgnn_module
            # This patch fixes common shape-mismatch issues in the LayerNorm layer
            def patched_ln_init(self, normalized_shape, eps=1e-05, elementwise_affine=True):
                if isinstance(normalized_shape, (list, tuple)):
                    normalized_shape = (normalized_shape[0],) if normalized_shape and normalized_shape[0] > 0 else (16,)
                torch.nn.Module.__init__(self)
                self.normalized_shape, self.eps, self.elementwise_affine = normalized_shape, eps, elementwise_affine
                if self.elementwise_affine:
                    self.weight = torch.nn.Parameter(torch.ones(normalized_shape))
                    self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))
            stgnn_module.LayerNorm.__init__ = patched_ln_init
            print(" STGNN LayerNorm Patched successfully")
        except Exception as e:
            print(f" Patching warning: {e}")

        # =====================================================
        # 3. ARGUMENT PARSING & EXECUTION
        # =====================================================
        parser = argparse.ArgumentParser()
        parser.add_argument('--data_path', type=str)
        parser.add_argument('--model', type=str)
        parser.add_argument('--config', type=str)
        parser.add_argument('--trained_model', type=str)
        parser.add_argument('--epoch_loss', type=str)
        args = parser.parse_args()

        # Ensure directory structure exists
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.epoch_loss), exist_ok=True)

        # Critical: Map DataWrapper to __main__ so pickle can resolve the class type
        import __main__
        __main__.DataWrapper = DataWrapper
        
        # Load Data
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)

        # Dynamic attribute detection (handles both .x and .x_train conventions)
        X = data.x if hasattr(data, 'x') else data.x_train
        Y = data.y if hasattr(data, 'y') else data.y_train
        X, Y = torch.as_tensor(X, dtype=torch.float32), torch.as_tensor(Y, dtype=torch.float32)
        
        # Load Config
        try:
            config = json.loads(args.config)
        except:
            # Handle cases where config might be a path or a literal string
            with open(args.config, 'r') as f:
                config = json.load(f)

        #  DIMENSION SHIFT: Enforce [Batch, Feature, Nodes, Time]
        # This solves the "Dimension -1" Conv2D error
        if len(X.shape) == 3:
            # Assume [Batch, Time, Nodes] -> [Batch, 1, Nodes, Time]
            X = X.permute(0, 2, 1).unsqueeze(1)
            Y = Y.permute(0, 2, 1).unsqueeze(1)
            print(f" Dimensions auto-corrected to: {X.shape}")

        config['num_nodes'] = X.shape[2]
        config['in_dim'] = X.shape[1]
        config['seq_in_len'] = X.shape[-1]

        # Initialize Model
        model = stgnn_module.STGNN(config)
        epoch_loss_data = []

        # =====================================================
        # 4. TRAINING LOOP
        # =====================================================
        try:
            if config.get('use_cafo'):
                print("Starting CAFO training...")
                st_wrapper = STGNNDataWrapper(X, Y, config.get('batch_size', 32))
                result = model.train_cafo(st_wrapper, verbose=True)
                for i, b_res in enumerate(result['block_results']):
                    for e, l in enumerate(b_res['train_losses']):
                        epoch_loss_data.append({'block': i+1, 'epoch': e+1, 'loss': float(l)})
            else:
                print("Starting standard training...")
                optimizer = torch.optim.Adam(model.parameters(), lr=config.get('learning_rate', 0.001))
                for e in range(config.get('epochs', 5)):
                    model.train()
                    optimizer.zero_grad()
                    output = model(X)
                    # Match temporal dimensions for loss calculation
                    target = Y[..., -output.shape[-1]:]
                    loss = F.mse_loss(output, target)
                    loss.backward()
                    optimizer.step()
                    epoch_loss_data.append({'epoch': e+1, 'loss': float(loss.item())})
                    print(f"Epoch {e+1}: Loss {loss.item():.6f}")

        except Exception as e:
            print(f" Training Error: {e}")
            sys.exit(1)

        # =====================================================
        # 5. SAVE OUTPUTS
        # =====================================================
        torch.save(model.state_dict(), args.trained_model)
        with open(args.epoch_loss, 'w') as f:
            json.dump({'epoch_losses': epoch_loss_data}, f)
        
        print("üèÅ Training Component Finished Successfully.")
    args:
      - --data_path
      - {inputPath: data_path}
      - --model
      - {inputPath: model}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
