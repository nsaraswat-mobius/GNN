name: V_Train_GNN_FF_Fixed_Final
description: Robust STGNN Training with Automatic Dimension Shifting for CAFO
inputs:
  - {name: data_path, type: Dataset}
  - {name: model, type: Model}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, sys, json, pickle, argparse, torch
        import numpy as np
        import torch.nn.functional as F

        # 1. Circular Import & LayerNorm Fixes
        original_import = __builtins__.__import__
        def block_torchvision(name, *args, **kwargs):
            if "torchvision" in name:
                import types
                return types.ModuleType(name)
            return original_import(name, *args, **kwargs)
        __builtins__.__import__ = block_torchvision

        from nesy_factory.GNNs import create_model
        import nesy_factory.GNNs.stgnn as stgnn_module
        __builtins__.__import__ = original_import

        def patched_ln_init(self, normalized_shape, eps=1e-05, elementwise_affine=True):
            if isinstance(normalized_shape, (list, tuple)):
                normalized_shape = (normalized_shape[0],) if normalized_shape and normalized_shape[0] > 0 else (16,)
            stgnn_module.nn.Module.__init__(self)
            self.normalized_shape, self.eps, self.elementwise_affine = normalized_shape, eps, elementwise_affine
            if self.elementwise_affine:
                self.weight = torch.nn.Parameter(torch.ones(normalized_shape))
                self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))
        stgnn_module.LayerNorm.__init__ = patched_ln_init

        # 2. Argument Parsing
        parser = argparse.ArgumentParser()
        for arg in ['--data_path', '--model', '--config', '--trained_model', '--epoch_loss']:
            parser.add_argument(arg, type=str)
        args = parser.parse_args()

        # 3. Load Data & Config
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)
        X = data.x if hasattr(data, 'x') else data.x_train
        Y = data.y if hasattr(data, 'y') else data.y_train
        X, Y = torch.as_tensor(X, dtype=torch.float32), torch.as_tensor(Y, dtype=torch.float32)
        config = json.loads(args.config)

        # 4. CRITICAL: DIMENSION SHIFT
        # Transform [Batch, Time, Nodes] -> [Batch, 1, Nodes, Time]
        if len(X.shape) == 3:
            X = X.permute(0, 2, 1).unsqueeze(1) 
            Y = Y.permute(0, 2, 1).unsqueeze(1)
            print(f" Shifted Dimensions: {X.shape}")

        # Ensure seq_in_len matches the data
        config['num_nodes'] = X.shape[2]
        config['in_dim'] = X.shape[1]
        config['seq_in_len'] = X.shape[-1]

        # 5. Initialize STGNN Model
        model = stgnn_module.STGNN(config)

        # 6. CAFO Training Wrapper
        class STGNNDataWrapper:
            def __init__(self, x, y, b_size):
                self.train_loader = self._create_loader(x, y, b_size)
                self.scaler = type('S', (), {'transform': lambda self, x: x, 'inverse_transform': lambda self, x: x})()
            def _create_loader(self, x, y, b):
                class L:
                    def __init__(self, x, y, b): self.x, self.y, self.b = x, y, b
                    def get_iterator(self):
                        def gen():
                            for i in range(0, len(self.x), self.b):
                                yield self.x[i:i+self.b].numpy(), self.y[i:i+self.b].numpy()
                        return gen()
                return L(x, y, b)

        epoch_loss_data = []
        try:
            if config.get('use_cafo'):
                print(" Running CAFO Training...")
                wrapper = STGNNDataWrapper(X, Y, config.get('batch_size', 64))
                result = model.train_cafo(wrapper, verbose=True)
                # Flatten CAFO results for epoch_loss output
                for i, b_res in enumerate(result['block_results']):
                    for e, l in enumerate(b_res['train_losses']):
                        epoch_loss_data.append({'block': i+1, 'epoch': e+1, 'loss': float(l)})
            else:
                # Standard Backprop Fallback
                opt = torch.optim.Adam(model.parameters(), lr=config.get('learning_rate', 0.0003))
                for e in range(config.get('epochs', 2)):
                    model.train(); opt.zero_grad()
                    out = model(X)
                    loss = F.mse_loss(out, Y[..., -out.shape[-1]:])
                    loss.backward(); opt.step()
                    epoch_loss_data.append({'epoch': e+1, 'loss': float(loss.item())})
        except Exception as e:
            print(f" Training Failed: {e}")

        # 7. Safe Save
        for p in [args.trained_model, args.epoch_loss]:
            os.makedirs(os.path.dirname(p), exist_ok=True)
        torch.save(model.state_dict(), args.trained_model)
        with open(args.epoch_loss, 'w') as f:
            json.dump({'epoch_losses': epoch_loss_data}, f)
        print(" Finished.")
    args:
      - --data_path
      - {inputPath: data_path}
      - --model
      - {inputPath: model}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
