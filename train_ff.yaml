name: V_Train_GNN_FF_Fixed
description: Trains STGNN with Backprop / CAFO / Forward-Forward (Bulletproof)
inputs:
  - { name: data_path, type: Dataset }
  - { name: model, type: Model }
  - { name: config, type: String }
outputs:
  - { name: trained_model, type: Model }
  - { name: epoch_loss, type: String }

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec python3 -u - << 'EOF'
        import os, sys, json, pickle, argparse
        import torch
        import numpy as np

        # =====================================================
        # BLOCK TORCHVISION (circular import fix)
        # =====================================================
        original_import = __builtins__.__import__
        def block_torchvision(name, *args, **kwargs):
            if "torchvision" in name:
                import types
                dummy = types.ModuleType(name)
                sys.modules[name] = dummy
                return dummy
            return original_import(name, *args, **kwargs)
        __builtins__.__import__ = block_torchvision

        # =====================================================
        # SAFE DUMMY CLASSES FOR PICKLE
        # =====================================================
        class DataWrapper: pass
        class DataLoaderM: pass
        class StandardScaler: pass

        # =====================================================
        # IMPORT STGNN
        # =====================================================
        sys.path.insert(0, "/app/src")
        try:
            from nesy_factory.GNNs.stgnn import STGNN
            STGNN_AVAILABLE = True
        except Exception as e:
            print("STGNN import failed:", e)
            STGNN_AVAILABLE = False

        __builtins__.__import__ = original_import

        # =====================================================
        # ARGUMENTS
        # =====================================================
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--model", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--epoch_loss", required=True)
        args = parser.parse_args()

        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.epoch_loss), exist_ok=True)

        # =====================================================
        # LOAD CONFIG
        # =====================================================
        config = json.loads(args.config)

        use_cafo = config.get("use_cafo", False)
        use_ff = config.get("use_forward_forward", False)

        training_mode = (
            "forward_forward" if use_ff else
            "cafo" if use_cafo else
            "backprop"
        )

        # =====================================================
        # BULLETPROOF CONFIG FIXES (NEGATIVE TENSOR FIX)
        # =====================================================
        config["num_nodes"] = int(config.get("num_nodes", 6))
        config["seq_in_len"] = max(12, int(config.get("seq_in_len", 100)))
        config["layers"] = max(1, int(config.get("layers", 2)))
        config["conv_channels"] = max(8, int(config.get("conv_channels", 16)))
        config["residual_channels"] = max(8, int(config.get("residual_channels", 16)))
        config["skip_channels"] = max(8, int(config.get("skip_channels", 32)))
        config["end_channels"] = max(8, int(config.get("end_channels", 64)))
        config["in_dim"] = int(config.get("in_dim", 1))
        config["dropout"] = float(config.get("dropout", 0.1))

        # =====================================================
        # LOAD DATA (ROBUST â€“ NO .x ASSUMPTION)
        # =====================================================
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)

        X = Y = None

        if hasattr(data, "x") and hasattr(data, "y"):
            X, Y = data.x, data.y

        elif isinstance(data, dict) and "x" in data and "y" in data:
            X, Y = data["x"], data["y"]

        else:
            for attr in vars(data).values():
                if isinstance(attr, dict) and "x" in attr and "y" in attr:
                    X, Y = attr["x"], attr["y"]
                    break
                if hasattr(attr, "x") and hasattr(attr, "y"):
                    X, Y = attr.x, attr.y
                    break

        if X is None or Y is None:
            raise RuntimeError(
                f"Unable to find x/y in dataset. Found attributes: {vars(data).keys()}"
            )

        X = torch.tensor(X) if not torch.is_tensor(X) else X
        Y = torch.tensor(Y) if not torch.is_tensor(Y) else Y

        # STGNN expects [B, C, N, T]
        if X.dim() == 4:
            X = X.permute(0, 3, 2, 1).contiguous()
        if Y.dim() == 4:
            Y = Y.permute(0, 3, 2, 1).contiguous()

        print("Data loaded:", X.shape, Y.shape)

        # =====================================================
        # CREATE MODEL
        # =====================================================
        if not STGNN_AVAILABLE:
            raise RuntimeError("STGNN not available in container")

        model = STGNN(config)

        # =====================================================
        # TRAINING
        # =====================================================
        losses = []

        if training_mode == "forward_forward" and hasattr(model, "train_forward_forward"):
            result = model.train_forward_forward((X, Y), verbose=True)
            for b in result["block_results"]:
                losses.extend(b["train_losses"])

        elif training_mode == "cafo" and hasattr(model, "train_cafo"):
            result = model.train_cafo((X, Y), verbose=True)
            for b in result["block_results"]:
                losses.extend(b["train_losses"])

        else:
            optimizer = torch.optim.Adam(model.parameters(), lr=config.get("learning_rate", 0.001))
            criterion = torch.nn.MSELoss()
            model.train()
            for _ in range(config.get("epochs", 2)):
                optimizer.zero_grad()
                out = model(X)
                loss = criterion(out, Y)
                loss.backward()
                optimizer.step()
                losses.append(float(loss.item()))

        # =====================================================
        # SAVE OUTPUTS
        # =====================================================
        torch.save(model.state_dict(), args.trained_model)

        with open(args.epoch_loss, "w") as f:
            json.dump({
                "training_mode": training_mode,
                "losses": losses,
                "final_loss": losses[-1] if losses else None
            }, f, indent=2)

        print(" TRAINING COMPLETED SUCCESSFULLY")
        EOF
    args:
      - --data_path
      - { inputPath: data_path }
      - --model
      - { inputPath: model }
      - --config
      - { inputValue: config }
      - --trained_model
      - { outputPath: trained_model }
      - --epoch_loss
      - { outputPath: epoch_loss }
