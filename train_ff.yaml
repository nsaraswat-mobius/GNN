name: V_Train_GNN_FF_Fixed
description: Trains STGNN with Backprop / CAFO / Forward-Forward 
inputs:
  - { name: data_path, type: Dataset }
  - { name: model, type: Model }
  - { name: config, type: String }
outputs:
  - { name: trained_model, type: Model }
  - { name: epoch_loss, type: String }

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec python3 -u - << 'EOF'
        import os, sys, json, pickle, argparse
        import torch
        import numpy as np

        # =====================================================
        # BLOCK TORCHVISION (circular import fix)
        # =====================================================
        original_import = __builtins__.__import__
        def block_torchvision(name, *args, **kwargs):
            if "torchvision" in name:
                import types
                dummy = types.ModuleType(name)
                sys.modules[name] = dummy
                return dummy
            return original_import(name, *args, **kwargs)
        __builtins__.__import__ = block_torchvision

        # =====================================================
        # REQUIRED DATA CLASSES FOR STGNN TRAINING
        # =====================================================
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)
        
        class StandardScaler:
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean
        
        class DataLoaderM:
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys
            
            def shuffle(self):
                permutation = np.random.permutation(self.size)
                xs, ys = self.xs[permutation], self.ys[permutation]
                self.xs = xs
                self.ys = ys
            
            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        x_i = self.xs[start_ind: end_ind, ...]
                        y_i = self.ys[start_ind: end_ind, ...]
                        yield (x_i, y_i)
                        self.current_ind += 1
                return _wrapper()
        
        class STGNNDataWrapper:
            def __init__(self, x_data, y_data, batch_size=32):
                self.x_data = x_data
                self.y_data = y_data
                self.scaler = StandardScaler(x_data.mean(), x_data.std())
                self.train_loader = DataLoaderM(x_data.numpy(), y_data.numpy(), batch_size)

        # =====================================================
        # IMPORT STGNN
        # =====================================================
        sys.path.insert(0, "/app/src")
        try:
            from nesy_factory.GNNs.stgnn import STGNN
            STGNN_AVAILABLE = True
        except Exception as e:
            print("STGNN import failed:", e)
            STGNN_AVAILABLE = False

        __builtins__.__import__ = original_import

        # =====================================================
        # ARGUMENTS
        # =====================================================
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--model", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--epoch_loss", required=True)
        args = parser.parse_args()

        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.epoch_loss), exist_ok=True)

        # =====================================================
        # LOAD CONFIG
        # =====================================================
        config = json.loads(args.config)

        use_cafo = config.get("use_cafo", False)
        use_ff = config.get("use_forward_forward", False)

        training_mode = (
            "forward_forward" if use_ff else
            "cafo" if use_cafo else
            "backprop"
        )

        # =====================================================
        # BULLETPROOF CONFIG FIXES (NEGATIVE TENSOR FIX)
        # =====================================================
        config["num_nodes"] = int(config.get("num_nodes", 6))
        config["seq_in_len"] = max(12, int(config.get("seq_in_len", 100)))
        config["layers"] = max(1, int(config.get("layers", 2)))
        config["conv_channels"] = max(8, int(config.get("conv_channels", 16)))
        config["residual_channels"] = max(8, int(config.get("residual_channels", 16)))
        config["skip_channels"] = max(8, int(config.get("skip_channels", 32)))
        config["end_channels"] = max(8, int(config.get("end_channels", 64)))
        config["in_dim"] = int(config.get("in_dim", 1))
        config["dropout"] = float(config.get("dropout", 0.1))
        
        # Add CAFO/FF specific config if needed
        if use_cafo:
            config["use_cafo"] = True
            config["cafo_blocks"] = config.get("cafo_blocks", 3)
            config["epochs_per_block"] = config.get("epochs_per_block", 50)
            config["block_lr"] = config.get("block_lr", 0.001)
        
        if use_ff:
            config["use_forward_forward"] = True
            config["ff_blocks"] = config.get("ff_blocks", 3)
            config["ff_threshold"] = config.get("ff_threshold", 2.0)
            config["ff_epochs_per_block"] = config.get("ff_epochs_per_block", 100)

        # =====================================================
        # LOAD DATA (ROBUST â€“ NO .x ASSUMPTION)
        # =====================================================
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)

        X = Y = None

        if hasattr(data, "x") and hasattr(data, "y"):
            X, Y = data.x, data.y

        elif isinstance(data, dict) and "x" in data and "y" in data:
            X, Y = data["x"], data["y"]

        else:
            for attr in vars(data).values():
                if isinstance(attr, dict) and "x" in attr and "y" in attr:
                    X, Y = attr["x"], attr["y"]
                    break
                if hasattr(attr, "x") and hasattr(attr, "y"):
                    X, Y = attr.x, attr.y
                    break

        if X is None or Y is None:
            raise RuntimeError(
                f"Unable to find x/y in dataset. Found attributes: {vars(data).keys()}"
            )

        X = torch.tensor(X, dtype=torch.float32) if not torch.is_tensor(X) else X.float()
        Y = torch.tensor(Y, dtype=torch.float32) if not torch.is_tensor(Y) else Y.float()

        # Convert to proper STGNN format [B, T, N, C] -> [B, C, N, T] if needed
        if X.dim() == 4:
            # Assuming input is [batch, time, nodes, features]
            if X.shape[1] == config["seq_in_len"]:
                X = X.permute(0, 3, 2, 1).contiguous()  # [B, T, N, C] -> [B, C, N, T]
            # If already [B, C, N, T], keep as is
            
        if Y.dim() == 4:
            if Y.shape[1] == config.get("seq_out_len", 1):
                Y = Y.permute(0, 3, 2, 1).contiguous()  # [B, T, N, C] -> [B, C, N, T]
        
        # Ensure minimum dimensions
        if X.dim() < 4:
            while X.dim() < 4:
                X = X.unsqueeze(-1)
        if Y.dim() < 4:
            while Y.dim() < 4:
                Y = Y.unsqueeze(-1)

        print("Data loaded and formatted:", X.shape, Y.shape)

        # =====================================================
        # CREATE MODEL
        # =====================================================
        if not STGNN_AVAILABLE:
            raise RuntimeError("STGNN not available in container")

        model = STGNN(config)

        # =====================================================
        # TRAINING WITH PROPER DATA WRAPPER
        # =====================================================
        losses = []

        if training_mode == "forward_forward" and hasattr(model, "train_forward_forward"):
            print(f" Starting Forward-Forward training...")
            # Create proper data wrapper for FF training
            stgnn_data = STGNNDataWrapper(X, Y, batch_size=config.get("batch_size", 32))
            result = model.train_forward_forward(stgnn_data, verbose=True)
            for b in result["block_results"]:
                losses.extend(b["train_losses"])
            print(f" Forward-Forward training completed!")

        elif training_mode == "cafo" and hasattr(model, "train_cafo"):
            print(f" Starting CAFO training...")
            # Create proper data wrapper for CAFO training
            stgnn_data = STGNNDataWrapper(X, Y, batch_size=config.get("batch_size", 32))
            result = model.train_cafo(stgnn_data, verbose=True)
            for b in result["block_results"]:
                losses.extend(b["train_losses"])
            print(f" CAFO training completed!")

        else:
            print(f" Starting traditional backprop training...")
            optimizer = torch.optim.Adam(model.parameters(), lr=config.get("learning_rate", 0.001))
            criterion = torch.nn.MSELoss()
            model.train()
            
            epochs = config.get("epochs", 2)
            for epoch in range(epochs):
                optimizer.zero_grad()
                out = model(X)
                
                # Handle output dimension alignment
                if out.shape != Y.shape:
                    print(f"  Output shape {out.shape} != target shape {Y.shape}, aligning...")
                    if out.dim() == 3 and Y.dim() == 4:
                        out = out.unsqueeze(-1)
                    elif out.shape[0] != Y.shape[0]:
                        min_batch = min(out.shape[0], Y.shape[0])
                        out = out[:min_batch]
                        Y = Y[:min_batch]
                
                loss = criterion(out, Y)
                loss.backward()
                optimizer.step()
                losses.append(float(loss.item()))
                
                if epoch % max(1, epochs // 5) == 0:
                    print(f"  Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.6f}")
            
            print(f" Traditional training completed!")

        # =====================================================
        # SAVE OUTPUTS
        # =====================================================
        torch.save(model.state_dict(), args.trained_model)

        with open(args.epoch_loss, "w") as f:
            json.dump({
                "training_mode": training_mode,
                "losses": losses,
                "final_loss": losses[-1] if losses else None
            }, f, indent=2)

        print(" TRAINING COMPLETED SUCCESSFULLY")
        EOF
    args:
      - --data_path
      - { inputPath: data_path }
      - --model
      - { inputPath: model }
      - --config
      - { inputValue: config }
      - --trained_model
      - { outputPath: trained_model }
      - --epoch_loss
      - { outputPath: epoch_loss }
