name: V_Train_GNN_FF_Fixed_Full
description: STGNN Training 
inputs:
  - {name: data_path, type: Dataset}
  - {name: model, type: Model}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, sys, json, pickle, argparse, torch
        import numpy as np
        import torch.nn.functional as F

        # =====================================================
        # KEEPING ALL YOUR ORIGINAL CLASSES
        # =====================================================
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean

        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys
            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        yield (self.xs[start_ind: end_ind, ...], self.ys[start_ind: end_ind, ...])
                        self.current_ind += 1
                return _wrapper()

        class STGNNDataWrapper:
            def __init__(self, x_data, y_data, batch_size=32):
                self.x_data = x_data
                self.y_data = y_data
                self.scaler = StandardScaler(x_data.mean(), x_data.std())
                self.train_loader = DataLoaderM(x_data.numpy(), y_data.numpy(), batch_size)

        # =====================================================
        # SYSTEM FIXES (Torchvision & LayerNorm)
        # =====================================================
        original_import = __builtins__.__import__
        def block_torchvision(name, *args, **kwargs):
            if "torchvision" in name:
                import types
                return types.ModuleType(name)
            return original_import(name, *args, **kwargs)
        __builtins__.__import__ = block_torchvision

        from nesy_factory.GNNs import create_model
        import nesy_factory.GNNs.stgnn as stgnn_module
        __builtins__.__import__ = original_import

        def patched_ln_init(self, normalized_shape, eps=1e-05, elementwise_affine=True):
            if isinstance(normalized_shape, (list, tuple)):
                normalized_shape = (normalized_shape[0],) if normalized_shape and normalized_shape[0] > 0 else (16,)
            stgnn_module.nn.Module.__init__(self)
            self.normalized_shape, self.eps, self.elementwise_affine = normalized_shape, eps, elementwise_affine
            if self.elementwise_affine:
                self.weight = torch.nn.Parameter(torch.ones(normalized_shape))
                self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))
        stgnn_module.LayerNorm.__init__ = patched_ln_init

        # =====================================================
        # MAIN EXECUTION
        # =====================================================
        parser = argparse.ArgumentParser()
        parser.add_argument('--data_path', type=str)
        parser.add_argument('--model', type=str)
        parser.add_argument('--config', type=str)
        parser.add_argument('--trained_model', type=str)
        parser.add_argument('--epoch_loss', type=str)
        args = parser.parse_args()

        # Pre-create dirs
        for p in [args.trained_model, args.epoch_loss]:
            os.makedirs(os.path.dirname(p), exist_ok=True)

        # Load Pickle with Class Mapping
        import __main__
        __main__.DataWrapper = DataWrapper
        with open(args.data_path, "rb") as f:
            data = pickle.load(f)

        X = data.x if hasattr(data, 'x') else data.x_train
        Y = data.y if hasattr(data, 'y') else data.y_train
        X, Y = torch.as_tensor(X, dtype=torch.float32), torch.as_tensor(Y, dtype=torch.float32)
        config = json.loads(args.config)

        #  DIMENSION SHIFT FIX: Ensure [Batch, Features, Nodes, Time]
        if len(X.shape) == 3:
            X = X.permute(0, 2, 1).unsqueeze(1) # Shift Time to last, add Feature dim
            Y = Y.permute(0, 2, 1).unsqueeze(1)
            print(f" Dimensions Shifted for STGNN: {X.shape}")

        #  RECEPTIVE FIELD PADDING FIX
        L, D, K = config.get('layers', 2), config.get('dilation_exponential', 1), config.get('kernel_size', 3)
        R = 1 + (K - 1) * (D**L - 1) // (D - 1) if D > 1 else L * (K - 1) + 1
        if X.shape[-1] < (R + 1):
            X = F.pad(X, (0, (R + 1) - X.shape[-1]))
            print(f" Padded sequence to satisfy Receptive Field")

        config['num_nodes'] = X.shape[2]
        config['in_dim'] = X.shape[1]
        config['seq_in_len'] = X.shape[-1]

        model = stgnn_module.STGNN(config)
        epoch_loss_data = []

        try:
            if config.get('use_cafo'):
                print(" Training with CAFO...")
                # Using your original STGNNDataWrapper class
                st_wrapper = STGNNDataWrapper(X, Y, config.get('batch_size', 64))
                result = model.train_cafo(st_wrapper, verbose=True)
                for i, b_res in enumerate(result['block_results']):
                    for e, l in enumerate(b_res['train_losses']):
                        epoch_loss_data.append({'block': i+1, 'epoch': e+1, 'loss': float(l)})
            else:
                optimizer = torch.optim.Adam(model.parameters(), lr=config.get('learning_rate', 0.0003))
                for e in range(config.get('epochs', 2)):
                    model.train(); optimizer.zero_grad()
                    output = model(X)
                    loss = F.mse_loss(output, Y[..., -output.shape[-1]:])
                    loss.backward(); optimizer.step()
                    epoch_loss_data.append({'epoch': e+1, 'loss': float(loss.item())})
                    print(f"Epoch {e+1} Loss: {loss.item()}")
        except Exception as e:
            print(f" Training Crash: {e}")

        # Save outputs
        torch.save(model.state_dict(), args.trained_model)
        with open(args.epoch_loss, 'w') as f:
            json.dump({'epoch_losses': epoch_loss_data}, f)
        print(" Process Completed.")
    args:
      - --data_path
      - {inputPath: data_path}
      - --model
      - {inputPath: model}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
