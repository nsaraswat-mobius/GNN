name: Load JSON dataset with anomaly detection
description: Fetches JSON from API.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import MinMaxScaler
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True, help='API URL to fetch JSON dataset')
        parser.add_argument('--access_token', type=str, required=True, help='Bearer token for API')
        parser.add_argument('--train_data', type=str, required=True, help='Path to output train dataset')
        parser.add_argument('--test_data', type=str, required=True, help='Path to output test dataset')
        parser.add_argument('--anomaly_data', type=str, required=True, help='Path to output anomaly labels')
        args = parser.parse_args()
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Fetch dataset from API
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        payload = {
            "dbType": "TIDB",
            "entityId": "",
            "entityIds": [],
            "ownedOnly": False,
            "projections": [],
            "filter": {},
            "startTime": 0,
            "endTime": 0
        }
        resp = requests.post(args.api_url, headers=headers, json=payload)
        resp.raise_for_status()
        raw_data = resp.json()
        
        df = pd.DataFrame(raw_data)
        
        # Flatten nested piMetadata dictionary into separate columns
        if "piMetadata" in df.columns:
            metadata_df = pd.json_normalize(df["piMetadata"])
            metadata_df.columns = [f"piMetadata.{col}" for col in metadata_df.columns]
            df = pd.concat([df.drop(columns=["piMetadata"]), metadata_df], axis=1)
        
        # Convert numeric columns properly
        numeric_cols = ["endpoint_count", "reachable_frac", "deg_cent", "betw_cent", "clustering", "pagerank"]
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # Handle anomalies: reachable_frac == 0.0 â†’ anomaly=1
        anomaly_labels = (df["reachable_frac"] == 0.0).astype(np.int32)
        
        # Normalize selected features
        scaler = MinMaxScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
        
        # Train-test split
        train_df, test_df, train_labels, test_labels = train_test_split(
            df, anomaly_labels, test_size=0.2, random_state=42, stratify=anomaly_labels
        )
        
        # Save train data
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        with open(args.train_data, "wb") as f:
            pickle.dump(train_df, f)
        
        # Save test data
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        with open(args.test_data, "wb") as f:
            pickle.dump(test_df, f)
        
        # Save anomaly labels for test set
        os.makedirs(os.path.dirname(args.anomaly_data) or ".", exist_ok=True)
        with open(args.anomaly_data, "wb") as f:
            pickle.dump(test_labels, f)
        
        print(" Successfully processed and saved dataset (from API).")
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputpath: access_token}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --anomaly_data
      - {outputPath: anomaly_data}
