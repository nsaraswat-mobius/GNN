name: Load JSON dataset for continual learning with STGNN compatibility
description: Fetches JSON from API and prepares sequential tasks for continual learning with STGNN, fully compatible with ContinualSTGNNTrainer and TemporalDataSplitter.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: num_tasks, type: int, description: 'Number of continual learning tasks to create', default: 3}
  - {name: splitting_strategy, type: string, description: 'Strategy for splitting data: temporal_split, anomaly_split, drift_split', default: 'temporal_split'}
  - {name: test_size, type: float, description: 'Fraction of data to use for testing', default: 0.2}
  - {name: val_size, type: float, description: 'Fraction of remaining data to use for validation', default: 0.2}
  - {name: batch_size, type: int, description: 'Batch size for data loaders', default: 32}
outputs:
  - {name: task_datasets, type: Dataset, description: 'List of task datasets for continual learning'}
  - {name: task_metadata, type: Dataset, description: 'Metadata for each continual learning task'}
  - {name: scaler_data, type: Dataset, description: 'Data scaler for consistent normalization across tasks'}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn numpy torch || \
        python3 -m pip install --quiet requests pandas scikit-learn numpy torch --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        import torch
        import json

        # Simple DataLoaderM implementation for compatibility
        class DataLoaderM:
            def __init__(self, x_data, y_data, batch_size):
                self.x_data = torch.FloatTensor(x_data) if not isinstance(x_data, torch.Tensor) else x_data
                self.y_data = torch.FloatTensor(y_data) if not isinstance(y_data, torch.Tensor) else y_data
                self.batch_size = batch_size
                self.num_samples = len(x_data)
                
            def __len__(self):
                return (self.num_samples + self.batch_size - 1) // self.batch_size
            
            def __iter__(self):
                for i in range(0, self.num_samples, self.batch_size):
                    end_idx = min(i + self.batch_size, self.num_samples)
                    yield self.x_data[i:end_idx], self.y_data[i:end_idx]

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True, help='API URL to fetch JSON dataset')
        parser.add_argument('--access_token', type=str, required=True, help='Bearer token for API')
        parser.add_argument('--num_tasks', type=int, default=3, help='Number of continual learning tasks')
        parser.add_argument('--splitting_strategy', type=str, default='temporal_split', 
                          help='Strategy: temporal_split, anomaly_split, drift_split')
        parser.add_argument('--test_size', type=float, default=0.2, help='Test set fraction')
        parser.add_argument('--val_size', type=float, default=0.2, help='Validation set fraction')
        parser.add_argument('--batch_size', type=int, default=32, help='Batch size for data loaders')
        parser.add_argument('--task_datasets', type=str, required=True, help='Path to output task datasets')
        parser.add_argument('--task_metadata', type=str, required=True, help='Path to output task metadata')
        parser.add_argument('--scaler_data', type=str, required=True, help='Path to output scaler data')
        args = parser.parse_args()

        print(f"🚀 Starting continual dataset loading with {args.splitting_strategy} strategy")
        print(f"Creating {args.num_tasks} tasks from API data...")

        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Fetch dataset from API - using exact payload from original brick
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        payload = {
            "dbType": "TIDB",
            "entityId": "",
            "entityIds": [],
            "ownedOnly": False,
            "projections": [],
            "filter": {},
            "startTime": 0,
            "endTime": 0
        }
        
        print("📡 Fetching data from API...")
        resp = requests.post(args.api_url, headers=headers, json=payload)
        resp.raise_for_status()
        raw_data = resp.json()
        
        df = pd.DataFrame(raw_data)
        print(f"📊 Loaded {len(df)} samples from API")
        
        # Drop unwanted columns - exact same as original brick
        drop_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid"]
        df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
        
        # Convert numeric columns properly - exact same as original brick
        numeric_cols = ["endpoint_count", "reachable_frac", "deg_cent", "betw_cent", "clustering", "pagerank"]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # Keep only numeric float features
        available_cols = [col for col in numeric_cols if col in df.columns]
        df = df[available_cols]
        
        print(f"🔧 Using features: {available_cols}")
        
        # Handle missing values and anomaly labels - matching original approach
        df = df.fillna(df.mean())
        
        # Handle anomalies: reachable_frac == 0.0 → anomaly=1 (from original brick)
        anomaly_labels = None
        if "reachable_frac" in df.columns:
            anomaly_labels = (df["reachable_frac"] == 0.0).astype(np.int32)
            print(f"🚨 Found {np.sum(anomaly_labels)} anomaly samples based on reachable_frac == 0.0")
        
        # For STGNN continual learning, we need time-series sequences
        # But first do train-test split on raw data (no normalization yet)
        train_df, test_df, train_anomalies, test_anomalies = train_test_split(
            df, anomaly_labels if anomaly_labels is not None else np.zeros(len(df)), 
            test_size=args.test_size, random_state=42, 
            stratify=anomaly_labels if anomaly_labels is not None else None
        )
        
        # Train-validation split
        val_size_adjusted = args.val_size / (1 - args.test_size)
        train_df_final, val_df, train_anomalies_final, val_anomalies = train_test_split(
            train_df, train_anomalies, test_size=val_size_adjusted, random_state=42,
            stratify=train_anomalies if anomaly_labels is not None else None
        )
        
        print(f"🎯 Data splits - Train: {len(train_df_final)}, Val: {len(val_df)}, Test: {len(test_df)}")
        
        # Fit scaler on training data only
        scaler = StandardScaler()
        scaler.fit(train_df_final)
        
        # Transform all datasets
        train_data_scaled = scaler.transform(train_df_final)
        val_data_scaled = scaler.transform(val_df)
        test_data_scaled = scaler.transform(test_df)
        
        # Convert to numpy arrays for task creation
        x_train = train_data_scaled
        y_train = train_data_scaled  # For autoencoder-style or next-step prediction
        x_val = val_data_scaled
        y_val = val_data_scaled
        x_test = test_data_scaled
        y_test = test_data_scaled
        
        print(f"📈 Prepared data - Train: {x_train.shape}, Val: {x_val.shape}, Test: {x_test.shape}")
        
        # Apply different splitting strategies for continual learning
        def temporal_split(x_train, y_train, x_val, y_val, x_test, y_test, num_tasks):
            """Split data chronologically into different time periods."""
            tasks = []
            
            # Calculate splits for each dataset
            train_splits = np.array_split(range(len(x_train)), num_tasks)
            val_splits = np.array_split(range(len(x_val)), num_tasks)
            test_splits = np.array_split(range(len(x_test)), num_tasks)
            
            for i in range(num_tasks):
                task_data = {
                    'task_id': i,
                    'x_train': x_train[train_splits[i]],
                    'y_train': y_train[train_splits[i]],
                    'x_val': x_val[val_splits[i]],
                    'y_val': y_val[val_splits[i]],
                    'x_test': x_test[test_splits[i]],
                    'y_test': y_test[test_splits[i]],
                    'scaler': scaler,  # Include scaler for compatibility
                    'description': f'Temporal Period {i+1}/{num_tasks}',
                    'strategy': 'temporal_split'
                }
                
                # Create data loaders for each task (matching example requirements)
                task_data['train_loader'] = DataLoaderM(
                    task_data['x_train'], task_data['y_train'], args.batch_size
                )
                task_data['val_loader'] = DataLoaderM(
                    task_data['x_val'], task_data['y_val'], args.batch_size
                )
                task_data['test_loader'] = DataLoaderM(
                    task_data['x_test'], task_data['y_test'], args.batch_size
                )
                
                # Add masks for compatibility (matching example requirements)
                task_data['train_mask'] = torch.ones(len(task_data['x_train']), dtype=torch.bool)
                task_data['test_mask'] = torch.ones(len(task_data['x_test']), dtype=torch.bool)
                
                tasks.append(task_data)
                
            return tasks
        
        def anomaly_split(x_train, y_train, x_val, y_val, x_test, y_test, num_tasks):
            """Split data based on anomaly patterns - simulating different anomaly types."""
            if num_tasks != 3:
                print("⚠️ Anomaly split currently supports exactly 3 tasks. Adjusting to 3.")
                num_tasks = 3
            
            tasks = []
            base_train_size = len(x_train) // num_tasks
            base_val_size = len(x_val) // num_tasks
            base_test_size = len(x_test) // num_tasks
            
            for i in range(num_tasks):
                # Select base data for this task
                train_start = i * base_train_size
                train_end = (i + 1) * base_train_size if i < num_tasks - 1 else len(x_train)
                
                val_start = i * base_val_size
                val_end = (i + 1) * base_val_size if i < num_tasks - 1 else len(x_val)
                
                test_start = i * base_test_size
                test_end = (i + 1) * base_test_size if i < num_tasks - 1 else len(x_test)
                
                x_train_task = x_train[train_start:train_end].copy()
                y_train_task = y_train[train_start:train_end].copy()
                x_val_task = x_val[val_start:val_end].copy()
                y_val_task = y_val[val_start:val_end].copy()
                x_test_task = x_test[test_start:test_end].copy()
                y_test_task = y_test[test_start:test_end].copy()
                
                # Inject different types of anomalies (matching example logic)
                if i == 1:  # Mild anomalies
                    anomaly_factor = 1.2
                    noise_level = 0.1
                    description = 'Mild Anomaly Pattern'
                elif i == 2:  # Severe anomalies
                    anomaly_factor = 1.5
                    noise_level = 0.2
                    description = 'Severe Anomaly Pattern'
                else:  # Normal data
                    anomaly_factor = 1.0
                    noise_level = 0.0
                    description = 'Normal Data Pattern'
                
                # Apply anomaly injection
                if anomaly_factor > 1.0:
                    # Random anomaly injection on training data
                    anomaly_indices = np.random.choice(
                        len(x_train_task), size=int(0.1 * len(x_train_task)), replace=False
                    )
                    x_train_task[anomaly_indices] *= anomaly_factor
                    if noise_level > 0:
                        noise = np.random.normal(0, noise_level, x_train_task[anomaly_indices].shape)
                        x_train_task[anomaly_indices] += noise
                
                task_data = {
                    'task_id': i,
                    'x_train': x_train_task,
                    'y_train': y_train_task,
                    'x_val': x_val_task,
                    'y_val': y_val_task,
                    'x_test': x_test_task,
                    'y_test': y_test_task,
                    'scaler': scaler,
                    'description': description,
                    'strategy': 'anomaly_split',
                    'anomaly_factor': anomaly_factor,
                    'noise_level': noise_level
                }
                
                # Create data loaders
                task_data['train_loader'] = DataLoaderM(
                    task_data['x_train'], task_data['y_train'], args.batch_size
                )
                task_data['val_loader'] = DataLoaderM(
                    task_data['x_val'], task_data['y_val'], args.batch_size
                )
                task_data['test_loader'] = DataLoaderM(
                    task_data['x_test'], task_data['y_test'], args.batch_size
                )
                
                # Add masks
                task_data['train_mask'] = torch.ones(len(task_data['x_train']), dtype=torch.bool)
                task_data['test_mask'] = torch.ones(len(task_data['x_test']), dtype=torch.bool)
                
                tasks.append(task_data)
                
            return tasks
        
        def drift_split(x_train, y_train, x_val, y_val, x_test, y_test, num_tasks):
            """Split data simulating concept drift - gradually changing data distribution."""
            tasks = []
            overlap_ratio = 0.2  # 20% overlap between consecutive tasks
            
            for i in range(num_tasks):
                # Progressive shift in data selection
                drift_factor = i / (num_tasks - 1) if num_tasks > 1 else 0
                
                # Calculate start and end indices with drift and overlap
                train_window_size = len(x_train) // num_tasks
                train_start = int(i * train_window_size * (1 - overlap_ratio))
                train_end = min(train_start + train_window_size, len(x_train))
                
                val_window_size = len(x_val) // num_tasks
                val_start = int(i * val_window_size * (1 - overlap_ratio))
                val_end = min(val_start + val_window_size, len(x_val))
                
                test_window_size = len(x_test) // num_tasks
                test_start = int(i * test_window_size * (1 - overlap_ratio))
                test_end = min(test_start + test_window_size, len(x_test))
                
                # Apply gradual drift to features
                x_train_task = x_train[train_start:train_end].copy()
                y_train_task = y_train[train_start:train_end].copy()
                x_val_task = x_val[val_start:val_end].copy()
                y_val_task = y_val[val_start:val_end].copy()
                x_test_task = x_test[test_start:test_end].copy()
                y_test_task = y_test[test_start:test_end].copy()
                
                # Apply concept drift (gradual shift in data distribution)
                drift_intensity = 0.1 * drift_factor  # Gradually increasing drift
                if drift_intensity > 0:
                    # Add systematic bias that increases over tasks
                    bias = np.sin(2 * np.pi * drift_factor) * drift_intensity
                    x_train_task += bias
                    x_val_task += bias
                    x_test_task += bias
                
                task_data = {
                    'task_id': i,
                    'x_train': x_train_task,
                    'y_train': y_train_task,
                    'x_val': x_val_task,
                    'y_val': y_val_task,
                    'x_test': x_test_task,
                    'y_test': y_test_task,
                    'scaler': scaler,
                    'description': f'Drift Period {i+1} (Intensity: {drift_intensity:.2f})',
                    'strategy': 'drift_split',
                    'drift_factor': drift_factor,
                    'drift_intensity': drift_intensity
                }
                
                # Create data loaders
                task_data['train_loader'] = DataLoaderM(
                    task_data['x_train'], task_data['y_train'], args.batch_size
                )
                task_data['val_loader'] = DataLoaderM(
                    task_data['x_val'], task_data['y_val'], args.batch_size
                )
                task_data['test_loader'] = DataLoaderM(
                    task_data['x_test'], task_data['y_test'], args.batch_size
                )
                
                # Add masks
                task_data['train_mask'] = torch.ones(len(task_data['x_train']), dtype=torch.bool)
                task_data['test_mask'] = torch.ones(len(task_data['x_test']), dtype=torch.bool)
                
                tasks.append(task_data)
                
            return tasks
        
        # Apply the selected splitting strategy
        print(f"🔄 Applying {args.splitting_strategy} strategy...")
        
        if args.splitting_strategy == 'temporal_split':
            tasks = temporal_split(x_train, y_train, x_val, y_val, x_test, y_test, args.num_tasks)
        elif args.splitting_strategy == 'anomaly_split':
            tasks = anomaly_split(x_train, y_train, x_val, y_val, x_test, y_test, args.num_tasks)
        elif args.splitting_strategy == 'drift_split':
            tasks = drift_split(x_train, y_train, x_val, y_val, x_test, y_test, args.num_tasks)
        else:
            raise ValueError(f"Unknown splitting strategy: {args.splitting_strategy}")
        
        # Create task metadata
        task_metadata = {
            'num_tasks': len(tasks),
            'splitting_strategy': args.splitting_strategy,
            'feature_columns': available_cols,
            'total_samples': len(df),
            'train_samples': len(x_train),
            'val_samples': len(x_val),
            'test_samples': len(x_test),
            'batch_size': args.batch_size,
            'task_descriptions': [task['description'] for task in tasks],
            'has_anomaly_labels': anomaly_labels is not None
        }
        
        print(f"✅ Created {len(tasks)} continual learning tasks:")
        for i, task in enumerate(tasks):
            print(f"  Task {i+1}: {task['description']} - Train: {len(task['x_train'])}, Val: {len(task['x_val'])}, Test: {len(task['x_test'])}")
        
        # Save task datasets
        os.makedirs(os.path.dirname(args.task_datasets) or ".", exist_ok=True)
        with open(args.task_datasets, "wb") as f:
            pickle.dump(tasks, f)
        
        # Save task metadata
        os.makedirs(os.path.dirname(args.task_metadata) or ".", exist_ok=True)
        with open(args.task_metadata, "wb") as f:
            pickle.dump(task_metadata, f)
        
        # Save scaler data (compatible with both original and continual learning)
        os.makedirs(os.path.dirname(args.scaler_data) or ".", exist_ok=True)
        scaler_info = {
            'scaler': scaler,
            'feature_columns': available_cols,
            'batch_size': args.batch_size
        }
        with open(args.scaler_data, "wb") as f:
            pickle.dump(scaler_info, f)
        
        print(f"💾 Successfully processed and saved continual learning dataset")
        print(f"   Strategy: {args.splitting_strategy}")
        print(f"   Tasks: {len(tasks)}")
        print(f"   Features: {len(available_cols)}")
        print(f"   Batch size: {args.batch_size}")
        print(f"   Has anomaly labels: {anomaly_labels is not None}")

    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --num_tasks
      - {inputValue: num_tasks}
      - --splitting_strategy
      - {inputValue: splitting_strategy}
      - --test_size
      - {inputValue: test_size}
      - --val_size
      - {inputValue: val_size}
      - --batch_size
      - {inputValue: batch_size}
      - --task_datasets
      - {outputPath: task_datasets}
      - --task_metadata
      - {outputPath: task_metadata}
      - --scaler_data
      - {outputPath: scaler_data}
