name: STGNN Continual Tasks Generator
description: Loads pickled STGNN dataset and splits it into continual learning tasks
inputs:
  - name: data_pickle
    type: Dataset
    description: "Pickle file containing DataWrapper object (STGNN formatted data)."
  - name: splitting_strategy
    type: String
    description: "Strategy for splitting data: temporal_split, anomaly_split, drift_split"
  - name: num_tasks
    type: Integer
    description: "Number of continual learning tasks to create"
outputs:
  - name: tasks_pickle
    type: Dataset
    description: "Pickle file containing list of continual learning tasks"
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet numpy torch || \
        python3 -m pip install --quiet numpy torch --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, pickle
        import numpy as np
        import torch
        class DataWrapper:
          def __init__(self, data_dict):
              self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean
        
        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys
        
            def shuffle(self):
                permutation = np.random.permutation(self.size)
                xs, ys = self.xs[permutation], self.ys[permutation]
                self.xs = xs
                self.ys = ys
        
            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        x_i = self.xs[start_ind: end_ind, ...]
                        y_i = self.ys[start_ind: end_ind, ...]
                        yield (x_i, y_i)
                        self.current_ind += 1
                return _wrapper()
        # import your splitter from the script
        class TemporalDataSplitter:
            
            def __init__(self, data, config, strategy='temporal_split'):
                self.data = data
                self.config = config
                self.strategy = strategy
                
            def create_continual_tasks(self, num_tasks: int = 3):
                if self.strategy == 'temporal_split':
                    return self._temporal_split(num_tasks)
                elif self.strategy == 'anomaly_split':
                    return self._anomaly_split(num_tasks)
                elif self.strategy == 'drift_split':
                    return self._drift_split(num_tasks)
                else:
                    raise ValueError(f"Unknown strategy: {self.strategy}")

            # (your _temporal_split, _anomaly_split, _drift_split go here unchanged,
            # indentation already fixed in the block you pasted)

        parser = argparse.ArgumentParser()
        parser.add_argument('--data_pickle', type=str, required=True, help='Input DataWrapper pickle path')
        parser.add_argument('--splitting_strategy', type=str, default='temporal_split', help='Split strategy')
        parser.add_argument('--num_tasks', type=int, default=3, help='Number of tasks to create')
        parser.add_argument('--tasks_pickle', type=str, required=True, help='Output pickle for tasks')
        args = parser.parse_args()

        # Load DataWrapper object
        with open(args.data_pickle, "rb") as f:
            data = pickle.load(f)

        # Split into continual tasks
        splitter = TemporalDataSplitter(data, config=getattr(data, "config", {}), strategy=args.splitting_strategy)
        tasks = splitter.create_continual_tasks(num_tasks=args.num_tasks)

        # Save tasks list as pickle
        os.makedirs(os.path.dirname(args.tasks_pickle) or ".", exist_ok=True)
        with open(args.tasks_pickle, "wb") as f:
            pickle.dump(tasks, f)

        print(f" Saved {len(tasks)} continual tasks {args.tasks_pickle}")
    args:
      - --data_pickle
      - {inputPath: data_pickle}
      - --splitting_strategy
      - {inputValue: splitting_strategy}
      - --num_tasks
      - {inputValue: num_tasks}
      - --tasks_pickle
      - {outputPath: tasks_pickle}
