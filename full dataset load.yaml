name: Load JSON dataset with pagination and anomaly detection
description: Fetches all JSON records from API via pagination and prepares train/test/anomaly datasets.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn || \
        python3 -m pip install --quiet requests pandas scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True, help='API URL to fetch JSON dataset')
        parser.add_argument('--access_token', type=str, required=True, help='Bearer token for API')
        parser.add_argument('--train_data', type=str, required=True, help='Path to output train dataset')
        parser.add_argument('--test_data', type=str, required=True, help='Path to output test dataset')
        parser.add_argument('--anomaly_data', type=str, required=True, help='Path to output anomaly labels')
        args = parser.parse_args()

        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Setup retry logger
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("api_pagination")

        # Setup session with retry logic
        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }

        # Pagination variables
        all_records = []
        limit = 2000
        offset = 0
        fetching = True

        logger.info(f"Starting paginated data fetch from {args.api_url}")

        while fetching:
            payload = {
                "dbType": "TIDB",
                "entityId": "",
                "entityIds": [],
                "ownedOnly": False,
                "projections": [],
                "filter": {},
                "startTime": 0,
                "endTime": 0,
                "limit": limit,
                "offset": offset
            }

            try:
                resp = session.post(args.api_url, headers=headers, json=payload, timeout=60)
                resp.raise_for_status()
                batch = resp.json()

                if not batch or len(batch) == 0:
                    logger.info("No more data found. Ending loop.")
                    fetching = False
                else:
                    all_records.extend(batch)
                    logger.info(f"Fetched {len(all_records)} total records (current offset: {offset})")
                    
                    # If we received fewer records than the limit, we've reached the end
                    if len(batch) < limit:
                        fetching = False
                    else:
                        offset += limit

            except requests.exceptions.RequestException as e:
                logger.error(f"API request failed at offset {offset}: {e}")
                raise

        if not all_records:
            raise ValueError("No data was returned from the API.")

        df = pd.DataFrame(all_records)
        
        # Drop unwanted columns
        drop_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid"]
        df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
        
        # Convert numeric columns properly
        numeric_cols = ["endpoint_count", "reachable_frac", "deg_cent", "betw_cent", "clustering", "pagerank"]
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # Replace NaN with 0
        df.fillna(0, inplace=True)
        
        # Keep only numeric float features
        df = df[numeric_cols]
        
        # Handle anomalies: reachable_frac == 0.0 â†’ anomaly=1
        anomaly_labels = (df["reachable_frac"] == 0.0).astype(np.int32)
        
        # Train-test split
        train_df, test_df, train_labels, test_labels = train_test_split(
            df, anomaly_labels, test_size=0.2, random_state=42, stratify=anomaly_labels
        )
        
        # Save results
        for path, data in [(args.train_data, train_df), (args.test_data, test_df), (args.anomaly_data, test_labels)]:
            os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            with open(path, "wb") as f:
                pickle.dump(data, f)
            
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --anomaly_data
      - {outputPath: anomaly_data}
