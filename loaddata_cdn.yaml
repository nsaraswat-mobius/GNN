name: Data loader with cdn v3

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download dataset file (CSV, JSON, or Parquet)'}
  - {name: split_size, type: Integer, default: "15", description: 'Test split size percent or fraction'}
  - {name: use_column, type: String, optional: true, description: 'Comma-separated columns to keep'}
  - {name: drop_columns, type: String, optional: true, description: 'Comma-separated columns to drop'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import tempfile
        import logging
        import json
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        try:
            from urllib3.util import Retry
        except Exception:
            from urllib3 import Retry
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import IsolationForest
        from scipy import stats

        # ---------- CLI ----------
        parser = argparse.ArgumentParser()
        parser.add_argument('--cdn_url', required=True)
        parser.add_argument('--split_size', type=float, default=15)
        parser.add_argument('--use_column', default=None)
        parser.add_argument('--drop_columns', default=None)
        parser.add_argument('--train_data', required=True)
        parser.add_argument('--test_data', required=True)
        parser.add_argument('--anomaly_data', required=True)
        args = parser.parse_args()

        # ---------- Handle $$ escaping ----------
        args.cdn_url = args.cdn_url.replace("$$", "$$$")

        # ---------- Logging ----------
        logging.basicConfig(
            stream=sys.stdout,
            level=logging.INFO,
            format="%(asctime)s %(levelname)s %(message)s"
        )
        logger = logging.getLogger("cdn_loader")

        # ---------- HTTP session ----------
        session = requests.Session()
        retry = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500,502,503,504],
            allowed_methods=frozenset(["GET","POST","HEAD"])
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ---------- Download ----------
        logger.info("Downloading dataset from CDN: %s", args.cdn_url)
        resp = session.get(args.cdn_url, timeout=30)
        resp.raise_for_status()

        fd, tmp_path = tempfile.mkstemp(suffix=".download")
        os.close(fd)
        with open(tmp_path, "wb") as f:
            f.write(resp.content)

        # ---------- Format detection ----------
        lower = args.cdn_url.lower()
        if lower.endswith(".parquet"):
            fmt = "parquet"
        elif lower.endswith(".csv"):
            fmt = "csv"
        elif lower.endswith(".json"):
            fmt = "json"
        else:
            ctype = resp.headers.get("Content-Type","").lower()
            if "parquet" in ctype:
                fmt = "parquet"
            elif "csv" in ctype or "text" in ctype:
                fmt = "csv"
            elif "json" in ctype:
                fmt = "json"
            else:
                fmt = None

        # ---------- Load ----------
        try:
            if fmt == "parquet":
                df = pd.read_parquet(tmp_path)
            elif fmt == "csv":
                df = pd.read_csv(tmp_path)
            elif fmt == "json":
                with open(tmp_path) as f:
                    raw = json.load(f)
                for key in ("data","items","results","records"):
                    if isinstance(raw, dict) and key in raw:
                        raw = raw[key]
                        break
                df = pd.json_normalize(raw if isinstance(raw, list) else [raw])
            else:
                raise ValueError("Unsupported dataset format")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

        logger.info("Dataset loaded: shape=%s", df.shape)
        logger.info("Columns: %s", df.columns.tolist())

        # ---------- COLUMN CONTROL ----------
        original_cols = set(df.columns)

        use_cols = (
            [c.strip() for c in args.use_column.split(",") if c.strip()]
            if args.use_column else None
        )

        drop_cols = (
            [c.strip() for c in args.drop_columns.split(",") if c.strip()]
            if args.drop_columns else []
        )

        if use_cols:
            missing = [c for c in use_cols if c not in df.columns]
            if missing:
                raise ValueError(
                    f"use_column contains missing columns: {missing}. "
                    f"Available columns: {list(df.columns)}"
                )
            df = df[use_cols]
            logger.info("use_column applied: %s", use_cols)

        if drop_cols:
            existing = [c for c in drop_cols if c in df.columns]
            df = df.drop(columns=existing, errors="ignore")
            logger.info("drop_columns applied: %s", existing)

        logger.info(
            "Final columns (%d): %s | Removed: %s",
            len(df.columns),
            df.columns.tolist(),
            list(original_cols - set(df.columns))
        )

        # Default anomaly detection settings
        anomaly_threshold = 0.05  # Fixed at 5%
        anomaly_method = "isolation_forest"  # Fixed method
        
        logger.info(f"Using anomaly detection method: {anomaly_method}")
        logger.info(f"Anomaly threshold: {anomaly_threshold:.1%}")

        # ---------- Split into train+test and anomalies ----------
        split = args.split_size / 100 if args.split_size >= 1 else args.split_size
        if not 0 < split < 1:
            raise ValueError("Invalid split_size")

        # Function to detect anomalies
        def detect_anomalies(data, method, contamination):
            if method == "isolation_forest":
                # Use Isolation Forest for anomaly detection
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    logger.warning("No numeric columns for Isolation Forest, using random")
                    return pd.Series([False] * len(data), index=data.index)
                
                X = data[numeric_cols].fillna(0)
                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=42,
                    n_estimators=100
                )
                predictions = iso_forest.fit_predict(X)
                return pd.Series(predictions == -1, index=data.index)
            
            elif method == "zscore":
                # Use Z-score method for numeric columns
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    logger.warning("No numeric columns for Z-score, using random")
                    return pd.Series([False] * len(data), index=data.index)
                
                # Calculate z-scores
                z_scores = np.abs(stats.zscore(data[numeric_cols].fillna(0)))
                # Mark as anomaly if any feature exceeds threshold (2.5 std dev)
                anomaly_mask = (z_scores > 2.5).any(axis=1)
                
                # Adjust to meet contamination rate if possible
                if anomaly_mask.sum() > len(data) * contamination:
                    # Too many anomalies, take top n
                    max_anomalies = int(len(data) * contamination)
                    anomaly_scores = z_scores.max(axis=1)
                    threshold = np.sort(anomaly_scores)[::-1][max_anomalies]
                    anomaly_mask = anomaly_scores >= threshold
                elif anomaly_mask.sum() < len(data) * contamination:
                    # Too few anomalies, add random ones
                    needed = int(len(data) * contamination) - anomaly_mask.sum()
                    if needed > 0:
                        non_anomaly_idx = data.index[~anomaly_mask]
                        extra_idx = np.random.choice(non_anomaly_idx, size=needed, replace=False)
                        anomaly_mask[extra_idx] = True
                
                return pd.Series(anomaly_mask, index=data.index)
            
            else:
                # Default to isolation_forest
                logger.warning(f"Using default isolation_forest method")
                return detect_anomalies(data, "isolation_forest", contamination)

        # Detect anomalies using fixed method and threshold
        is_anomaly = detect_anomalies(df, anomaly_method, anomaly_threshold)
        anomaly_df = df[is_anomaly]
        normal_df = df[~is_anomaly]

        logger.info(f"Detected {len(anomaly_df)} anomalies out of {len(df)} total samples")

        # Split normal data into train and test
        if len(normal_df) > 0:
            try:
                train_df, test_df = train_test_split(
                    normal_df, 
                    test_size=split, 
                    random_state=42, 
                    stratify=None
                )
            except ValueError:
                logger.warning("Train-test split failed, retrying")
                train_df, test_df = train_test_split(
                    normal_df, 
                    test_size=split, 
                    random_state=42, 
                    stratify=None
                )
        else:
            logger.warning("No normal data after anomaly detection")
            train_df = pd.DataFrame(columns=df.columns)
            test_df = pd.DataFrame(columns=df.columns)

        # Add anomaly flag to all datasets
        train_df['is_anomaly'] = 0
        test_df['is_anomaly'] = 0
        anomaly_df['is_anomaly'] = 1

        # ---------- Save ----------
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.anomaly_data) or ".", exist_ok=True)

        train_df.to_parquet(args.train_data, index=False)
        test_df.to_parquet(args.test_data, index=False)
        anomaly_df.to_parquet(args.anomaly_data, index=False)

        logger.info("Data split completed:")
        logger.info(f"  - Train data: {len(train_df)} samples")
        logger.info(f"  - Test data: {len(test_df)} samples")
        logger.info(f"  - Anomaly data: {len(anomaly_df)} samples")
        logger.info("Finished processing CDN dataset successfully")

    args:
      - --cdn_url
      - {inputValue: cdn_url}
      - --split_size
      - {inputValue: split_size}
      - --use_column
      - {inputValue: use_column}
      - --drop_columns
      - {inputValue: drop_columns}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --anomaly_data
      - {outputPath: anomaly_data}
