name: Data loader with cdn

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download dataset file (CSV, JSON, or Parquet)'}
  - {name: split_size, type: Integer, default: "15", description: 'Test split size percent or fraction'}
  - {name: target_column, type: String, description: 'Target column or comma-separated targets'}
  - {name: model_type, type: String, default: "classification", description: 'classification or regression'}
  - {name: anomaly_threshold, type: Float, default: "0.05", description: 'Percentage of data to isolate as anomalies (0-1)'}
  - {name: use_column, type: String, optional: true, description: 'Comma-separated columns to keep'}
  - {name: drop_columns, type: String, optional: true, description: 'Comma-separated columns to drop'}
  - {name: anomaly_detection_method, type: String, default: "random", description: 'Method for anomaly detection: random, isolation_forest, zscore, iqr'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import tempfile
        import logging
        import json
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        try:
            from urllib3.util import Retry
        except Exception:
            from urllib3 import Retry
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import IsolationForest
        from scipy import stats

        # ---------- CLI ----------
        parser = argparse.ArgumentParser()
        parser.add_argument('--cdn_url', required=True)
        parser.add_argument('--split_size', type=float, default=15)
        parser.add_argument('--target_column', required=True)
        parser.add_argument('--model_type', default="classification")
        parser.add_argument('--anomaly_threshold', type=float, default=0.05)
        parser.add_argument('--use_column', default=None)
        parser.add_argument('--drop_columns', default=None)
        parser.add_argument('--anomaly_detection_method', default="random")
        parser.add_argument('--train_data', required=True)
        parser.add_argument('--test_data', required=True)
        parser.add_argument('--anomaly_data', required=True)
        args = parser.parse_args()

        # ---------- Logging ----------
        logging.basicConfig(
            stream=sys.stdout,
            level=logging.INFO,
            format="%(asctime)s %(levelname)s %(message)s"
        )
        logger = logging.getLogger("cdn_loader")

        # ---------- HTTP session ----------
        session = requests.Session()
        retry = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500,502,503,504],
            allowed_methods=frozenset(["GET","POST","HEAD"])
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ---------- Download ----------
        logger.info("Downloading dataset from CDN: %s", args.cdn_url)
        resp = session.get(args.cdn_url, timeout=30)
        resp.raise_for_status()

        fd, tmp_path = tempfile.mkstemp(suffix=".download")
        os.close(fd)
        with open(tmp_path, "wb") as f:
            f.write(resp.content)

        # ---------- Format detection ----------
        lower = args.cdn_url.lower()
        if lower.endswith(".parquet"):
            fmt = "parquet"
        elif lower.endswith(".csv"):
            fmt = "csv"
        elif lower.endswith(".json"):
            fmt = "json"
        else:
            ctype = resp.headers.get("Content-Type","").lower()
            if "parquet" in ctype:
                fmt = "parquet"
            elif "csv" in ctype or "text" in ctype:
                fmt = "csv"
            elif "json" in ctype:
                fmt = "json"
            else:
                fmt = None

        # ---------- Load ----------
        try:
            if fmt == "parquet":
                df = pd.read_parquet(tmp_path)
            elif fmt == "csv":
                df = pd.read_csv(tmp_path)
            elif fmt == "json":
                with open(tmp_path) as f:
                    raw = json.load(f)
                for key in ("data","items","results","records"):
                    if isinstance(raw, dict) and key in raw:
                        raw = raw[key]
                        break
                df = pd.json_normalize(raw if isinstance(raw, list) else [raw])
            else:
                raise ValueError("Unsupported dataset format")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

        logger.info("Dataset loaded: shape=%s", df.shape)
        logger.info("Columns: %s", df.columns.tolist())

        # ---------- COLUMN CONTROL ----------
        original_cols = set(df.columns)

        use_cols = (
            [c.strip() for c in args.use_column.split(",") if c.strip()]
            if args.use_column else None
        )

        drop_cols = (
            [c.strip() for c in args.drop_columns.split(",") if c.strip()]
            if args.drop_columns else []
        )

        if use_cols:
            missing = [c for c in use_cols if c not in df.columns]
            if missing:
                raise ValueError(
                    f"use_column contains missing columns: {missing}. "
                    f"Available columns: {list(df.columns)}"
                )
            df = df[use_cols]
            logger.info("use_column applied: %s", use_cols)

        if drop_cols:
            existing = [c for c in drop_cols if c in df.columns]
            df = df.drop(columns=existing, errors="ignore")
            logger.info("drop_columns applied: %s", existing)

        logger.info(
            "Final columns (%d): %s | Removed: %s",
            len(df.columns),
            df.columns.tolist(),
            list(original_cols - set(df.columns))
        )

        # ---------- Target handling ----------
        target_cols = [c.strip() for c in args.target_column.split(",") if c.strip()]
        missing_targets = [c for c in target_cols if c not in df.columns]
        if missing_targets:
            raise ValueError(f"Target columns missing: {missing_targets}")

        target_df = df[target_cols]

        stratify = None
        if args.model_type == "classification" and len(target_cols) == 1:
            if target_df[target_cols[0]].nunique() > 1:
                stratify = target_df[target_cols[0]]

        # ---------- Split into train+test and anomalies ----------
        split = args.split_size / 100 if args.split_size >= 1 else args.split_size
        if not 0 < split < 1:
            raise ValueError("Invalid split_size")

        # First, separate anomalies
        anomaly_threshold = max(0.01, min(0.5, args.anomaly_threshold))  # Clamp between 1% and 50%
        
        logger.info(f"Using anomaly detection method: {args.anomaly_detection_method}")
        logger.info(f"Anomaly threshold: {anomaly_threshold:.1%}")
        
        # Function to detect anomalies
        def detect_anomalies(data, method, contamination):
            if method == "random":
                # Random selection
                n_anomalies = int(len(data) * contamination)
                if n_anomalies == 0:
                    return pd.Series([False] * len(data), index=data.index)
                anomaly_idx = np.random.choice(data.index, size=n_anomalies, replace=False)
                return pd.Series(data.index.isin(anomaly_idx), index=data.index)
            
            elif method == "isolation_forest":
                # Use Isolation Forest for anomaly detection
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    logger.warning("No numeric columns for Isolation Forest, using random")
                    return detect_anomalies(data, "random", contamination)
                
                X = data[numeric_cols].fillna(0)
                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=42,
                    n_estimators=100
                )
                predictions = iso_forest.fit_predict(X)
                return pd.Series(predictions == -1, index=data.index)
            
            elif method == "zscore":
                # Use Z-score method for numeric columns
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    logger.warning("No numeric columns for Z-score, using random")
                    return detect_anomalies(data, "random", contamination)
                
                # Calculate z-scores
                z_scores = np.abs(stats.zscore(data[numeric_cols].fillna(0)))
                # Mark as anomaly if any feature exceeds threshold (2.5 std dev)
                anomaly_mask = (z_scores > 2.5).any(axis=1)
                
                # Adjust to meet contamination rate if possible
                if anomaly_mask.sum() > len(data) * contamination:
                    # Too many anomalies, take top n
                    max_anomalies = int(len(data) * contamination)
                    anomaly_scores = z_scores.max(axis=1)
                    threshold = np.sort(anomaly_scores)[::-1][max_anomalies]
                    anomaly_mask = anomaly_scores >= threshold
                elif anomaly_mask.sum() < len(data) * contamination:
                    # Too few anomalies, add random ones
                    needed = int(len(data) * contamination) - anomaly_mask.sum()
                    if needed > 0:
                        non_anomaly_idx = data.index[~anomaly_mask]
                        extra_idx = np.random.choice(non_anomaly_idx, size=needed, replace=False)
                        anomaly_mask[extra_idx] = True
                
                return pd.Series(anomaly_mask, index=data.index)
            
            elif method == "iqr":
                # Use IQR method for numeric columns
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    logger.warning("No numeric columns for IQR, using random")
                    return detect_anomalies(data, "random", contamination)
                
                anomaly_mask = pd.Series([False] * len(data), index=data.index)
                
                for col in numeric_cols:
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    col_anomalies = (data[col] < lower_bound) | (data[col] > upper_bound)
                    anomaly_mask = anomaly_mask | col_anomalies
                
                # Adjust to meet contamination rate if possible
                if anomaly_mask.sum() > len(data) * contamination:
                    max_anomalies = int(len(data) * contamination)
                    # Use combination of features exceeding bounds
                    anomaly_scores = anomaly_mask.astype(int)
                    for col in numeric_cols:
                        Q1 = data[col].quantile(0.25)
                        Q3 = data[col].quantile(0.75)
                        IQR = Q3 - Q1
                        col_score = ((data[col] - Q1).abs() + (data[col] - Q3).abs()) / IQR
                        anomaly_scores += col_score.fillna(0)
                    
                    threshold = np.sort(anomaly_scores)[::-1][max_anomalies]
                    anomaly_mask = anomaly_scores >= threshold
                elif anomaly_mask.sum() < len(data) * contamination:
                    needed = int(len(data) * contamination) - anomaly_mask.sum()
                    if needed > 0:
                        non_anomaly_idx = data.index[~anomaly_mask]
                        extra_idx = np.random.choice(non_anomaly_idx, size=needed, replace=False)
                        anomaly_mask[extra_idx] = True
                
                return pd.Series(anomaly_mask, index=data.index)
            
            else:
                logger.warning(f"Unknown anomaly detection method: {method}, using random")
                return detect_anomalies(data, "random", contamination)

        # Detect anomalies
        is_anomaly = detect_anomalies(df, args.anomaly_detection_method, anomaly_threshold)
        anomaly_df = df[is_anomaly]
        normal_df = df[~is_anomaly]

        logger.info(f"Detected {len(anomaly_df)} anomalies out of {len(df)} total samples")

        # Split normal data into train and test
        if len(normal_df) > 0:
            try:
                # Adjust split size for remaining normal data
                if stratify is not None:
                    normal_stratify = stratify[~is_anomaly]
                else:
                    normal_stratify = None
                
                train_df, test_df = train_test_split(
                    normal_df, 
                    test_size=split, 
                    random_state=42, 
                    stratify=normal_stratify
                )
            except ValueError:
                logger.warning("Stratified split failed, retrying without stratification")
                train_df, test_df = train_test_split(
                    normal_df, 
                    test_size=split, 
                    random_state=42, 
                    stratify=None
                )
        else:
            logger.warning("No normal data after anomaly detection")
            train_df = pd.DataFrame(columns=df.columns)
            test_df = pd.DataFrame(columns=df.columns)

        # Add anomaly flag to all datasets
        train_df['is_anomaly'] = 0
        test_df['is_anomaly'] = 0
        anomaly_df['is_anomaly'] = 1

        # ---------- Save ----------
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.anomaly_data) or ".", exist_ok=True)

        train_df.to_parquet(args.train_data, index=False)
        test_df.to_parquet(args.test_data, index=False)
        anomaly_df.to_parquet(args.anomaly_data, index=False)

        logger.info("Data split completed:")
        logger.info(f"  - Train data: {len(train_df)} samples")
        logger.info(f"  - Test data: {len(test_df)} samples")
        logger.info(f"  - Anomaly data: {len(anomaly_df)} samples")
        logger.info("Finished processing CDN dataset successfully")

    args:
      - --cdn_url
      - {inputValue: cdn_url}
      - --split_size
      - {inputValue: split_size}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --anomaly_threshold
      - {inputValue: anomaly_threshold}
      - --use_column
      - {inputValue: use_column}
      - --drop_columns
      - {inputValue: drop_columns}
      - --anomaly_detection_method
      - {inputValue: anomaly_detection_method}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --anomaly_data
      - {outputPath: anomaly_data}
