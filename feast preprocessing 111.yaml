name: Preprocess dataset 2
description: Takes raw SMD data, runs the preprocessing script, and outputs a pickled DataWrapper object with the processed data.
inputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}
outputs:
  - {name: processed_data_pickle, type: Dataset}
  - {name: weight_out, type: String, description: "Dummy weights as JSON string"}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        apt-get update && apt-get install -y git
        pip install pandas scikit-learn numpy joblib torch
        
        # First, let's debug the input files
        echo "Checking input file formats..."
        python3 -c "
        import pickle
        import os
        print('Current directory:', os.getcwd())
        print('Files in current dir:', os.listdir('.'))
        print('Files in input paths:')
        import sys
        for i, path in enumerate(['$0', '$1', '$2']):
            print(f'Input {i}: {path}')
            if os.path.exists(path):
                print(f'  File exists, size: {os.path.getsize(path)} bytes')
                try:
                    with open(path, 'rb') as f:
                        # Try to peek at the file content
                        import binascii
                        header = f.read(100)
                        print(f'  First 100 bytes (hex): {binascii.hexlify(header)}')
                        print(f'  First 100 bytes (ascii): {header}')
                except Exception as e:
                    print(f'  Error reading: {e}')
            else:
                print(f'  File does not exist')
        "

        # Create and change to a temporary working directory
        TMP_DIR=$(mktemp -d -t cst-gl-XXXXXXXXXX)
        cd "$TMP_DIR"

        # Clone the repository that contains the generate_training_data.py script
        git clone https://github.com/huankoh/CST-GL
        cd CST-GL

        # The input data from the previous component will be available at these paths
        # We need to move them to where the script expects them.
        mkdir -p generate_data
        
        # First, let's try to convert the input files to simple numpy arrays
        cat > convert_inputs.py << 'EOF2'
        import pickle
        import numpy as np
        import torch
        import os
        import sys
        
        def convert_file(input_path, output_path):
            print(f"Converting {input_path} to {output_path}")
            
            # Try multiple loading methods
            data = None
            
            # Method 1: Try torch.load
            try:
                print("  Trying torch.load...")
                data = torch.load(input_path, map_location='cpu', pickle_module=pickle)
                if isinstance(data, torch.Tensor):
                    data = data.cpu().numpy()
                print(f"  Success with torch.load, type: {type(data)}, shape: {data.shape if hasattr(data, 'shape') else 'N/A'}")
            except Exception as e:
                print(f"  torch.load failed: {e}")
            
            # Method 2: Try standard pickle
            if data is None:
                try:
                    print("  Trying standard pickle...")
                    with open(input_path, 'rb') as f:
                        data = pickle.load(f)
                    if isinstance(data, torch.Tensor):
                        data = data.cpu().numpy()
                    print(f"  Success with pickle, type: {type(data)}")
                except Exception as e:
                    print(f"  pickle failed: {e}")
            
            # Method 3: Try joblib
            if data is None:
                try:
                    print("  Trying joblib...")
                    import joblib
                    data = joblib.load(input_path)
                    if isinstance(data, torch.Tensor):
                        data = data.cpu().numpy()
                    print(f"  Success with joblib, type: {type(data)}")
                except Exception as e:
                    print(f"  joblib failed: {e}")
            
            # Convert to numpy array if possible
            if data is not None:
                if not isinstance(data, np.ndarray):
                    try:
                        data = np.array(data, dtype=np.float32)
                    except:
                        print(f"  Warning: Could not convert to numpy array, type: {type(data)}")
                        return False
                
                # Save as simple numpy array
                with open(output_path, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
                print(f"  Saved to {output_path}, shape: {data.shape}")
                return True
            else:
                print(f"  Error: Could not load {input_path}")
                return False
        
        # Convert all input files
        input_files = [
            (sys.argv[1], 'generate_data/machine-1-1_train.pkl'),
            (sys.argv[2], 'generate_data/machine-1-1_test.pkl'),
            (sys.argv[3], 'generate_data/machine-1-1_test_label.pkl')
        ]
        
        for inp, out in input_files:
            if not convert_file(inp, out):
                print(f"Failed to convert {inp}")
                sys.exit(1)
        EOF2
        
        python convert_inputs.py "$0" "$1" "$2"

        # Verify the converted files
        echo "Verifying converted files..."
        python3 -c "
        import pickle
        import numpy as np
        import os
        
        files = ['generate_data/machine-1-1_train.pkl', 
                 'generate_data/machine-1-1_test.pkl', 
                 'generate_data/machine-1-1_test_label.pkl']
        
        for f in files:
            if os.path.exists(f):
                with open(f, 'rb') as fp:
                    data = pickle.load(fp)
                print(f'{f}: type={type(data)}, shape={data.shape if hasattr(data, \"shape\") else \"N/A\"}, dtype={data.dtype if hasattr(data, \"dtype\") else \"N/A\"}')
            else:
                print(f'{f}: File not found')
        "

        # The script has a bug, fix it with sed
        sed -i 's/train\.append(\[/pd.concat([train,/g' generate_data/generate_training_data.py

        # Create the output directory for the script
        mkdir -p data/machine-1-1

        # Run the preprocessing script
        python generate_data/generate_training_data.py \
          --output_dir 'data/machine-1-1' \
          --train_path 'generate_data/machine-1-1_train.pkl' \
          --test_path 'generate_data/machine-1-1_test.pkl' \
          --anomaly_path 'generate_data/machine-1-1_test_label.pkl' \
          --window_size 100 \
          --val_ratio 0.3

        # --- New Python script to create and pickle DataWrapper object ---
        cat > create_data_pickle.py << EOF
        import numpy as np
        import os
        import pickle
        import pandas as pd

        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean

        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                    y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                    xs = np.concatenate([xs, x_padding], axis=0)
                    ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys

            def shuffle(self):
                permutation = np.random.permutation(self.size)
                xs, ys = self.xs[permutation], self.ys[permutation]
                self.xs = xs
                self.ys = ys

            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        x_i = self.xs[start_ind: end_ind, ...]
                        y_i = self.ys[start_ind: end_ind, ...]
                        yield (x_i, y_i)
                        self.current_ind += 1
                return _wrapper()

        def create_sliding_windows(data, seq_in_len, seq_out_len):
            x, y = [], []
            num_samples = len(data)
            for i in range(num_samples - seq_in_len - seq_out_len + 1):
                x.append(data[i : i + seq_in_len])
                y.append(data[i + seq_in_len : i + seq_in_len + seq_out_len])
            return np.array(x), np.array(y)

        def load_dataset(dataset_dir, config, scaling_required=True):
            batch_size = config['batch_size']
            seq_in_len = config['seq_in_len']
            seq_out_len = config['seq_out_len']

            # Load the pre-converted numpy arrays
            with open(os.path.join(dataset_dir, 'machine-1-1_train.pkl'), 'rb') as f:
                train_raw = pickle.load(f)
            train_raw = np.array(train_raw, dtype=np.float32)

            with open(os.path.join(dataset_dir, 'machine-1-1_test.pkl'), 'rb') as f:
                test_raw = pickle.load(f)
            test_raw = np.array(test_raw, dtype=np.float32)

            x_train, y_train = create_sliding_windows(train_raw, seq_in_len, seq_out_len)
            x_test, y_test = create_sliding_windows(test_raw, seq_in_len, seq_out_len)

            # Add feature dimension
            x_train = np.expand_dims(x_train, axis=-1)
            y_train = np.expand_dims(y_train, axis=-1)
            x_test = np.expand_dims(x_test, axis=-1)
            y_test = np.expand_dims(y_test, axis=-1)

            # For simplicity, we'll use part of the training data as validation
            num_train = int(len(x_train) * 0.8)
            x_val, y_val = x_train[num_train:], y_train[num_train:]
            x_train, y_train = x_train[:num_train], y_train[:num_train]

            data = {}
            data['x_train'], data['y_train'] = x_train, y_train
            data['x_val'], data['y_val'] = x_val, y_val
            data['x_test'], data['y_test'] = x_test, y_test
            
            scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())

            if scaling_required:
                for category in ['train', 'val', 'test']:
                    data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])

            data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)
            data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], batch_size)
            data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], batch_size)
            data['scaler'] = scaler
            
            # Add dummy masks for compatibility with TGCN example structure
            data['train_mask'] = np.ones(len(data['x_train']), dtype=bool)
            data['test_mask'] = np.ones(len(data['x_test']), dtype=bool)
            
            return data

        if __name__ == '__main__':
            # Hardcoded config values based on typical STGNN setup and existing generate_training_data.py
            config = {
                'batch_size': 64,
                'seq_in_len': 100,
                'seq_out_len': 1
            }
            
            # The dataset_dir is where generate_training_data.py outputs its files
            dataset_dir = 'generate_data'
            
            data_dict = load_dataset(dataset_dir, config)
            data = DataWrapper(data_dict)
            
            output_path = os.environ['PROCESSED_DATA_PICKLE_PATH']
            
            # Ensure the output directory exists
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)

            # Save the shape of x_train for the next script
            with open('x_train_shape.txt', 'w') as f:
                f.write(','.join(map(str, data.x_train.shape)))
        EOF
        
        export PROCESSED_DATA_PICKLE_PATH="$3"
        python create_data_pickle.py

        # --- New Python script to create and save dummy weights ---
        cat > create_dummy_weights.py << EOF
        import json
        import os

        if __name__ == '__main__':
            # Read the shape of x_train from the file
            with open('x_train_shape.txt', 'r') as f:
                shape_str = f.read()
            x_train_shape = [int(s) for s in shape_str.split(',')]
            print(f"x_train shape: {x_train_shape}")
            num_nodes = x_train_shape[2]
            subgraph_size = min(num_nodes, 10)

            dummy_weights = {
                "focal_loss_alpha": 0.5,
                "class_weights": [1.0, 1.0, 1.0],
                "num_nodes": num_nodes,
                "subgraph_size": subgraph_size
            }
            output_path = os.environ['WEIGHT_OUT_PATH']
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w') as f:
                json.dump(dummy_weights, f, indent=2)
        EOF
        
        export WEIGHT_OUT_PATH="$4"
        python create_dummy_weights.py

        echo "Preprocessing completed successfully!"

    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: anomaly_data}
      - {outputPath: processed_data_pickle}
      - {outputPath: weight_out}
