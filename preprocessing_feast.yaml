name: Preprocess v6
description: Preprocesses Kubernetes metrics for GNN anomaly detection - handles edge cases.
inputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: anomaly_data, type: Dataset}
  - {name: window_size, type: Integer, default: "10", description: 'Window size for time series segmentation'}
outputs:
  - {name: processed_data_pickle, type: Dataset}
  - {name: metadata_json, type: String, description: "Metadata about processed dataset"}
implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        set -e
        apt-get update && apt-get install -y --no-install-recommends wget
        pip install pandas scikit-learn numpy pyarrow scipy joblib
        
        # --- Fixed preprocessing script with compatibility attributes ---
        cat > preprocess_gnn.py << 'EOF'
        import argparse
        import os
        import pickle
        import json
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        from sklearn.neighbors import kneighbors_graph
        
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)
        
        def create_sliding_windows(data, window_size, stride=1):
            if len(data) < window_size:
                # Pad with zeros
                padding = np.zeros((window_size - len(data), data.shape[1]), dtype=data.dtype)
                data = np.vstack([data, padding])
                return np.array([data])
            
            sequences = []
            num_samples = len(data)
            
            for i in range(0, num_samples - window_size + 1, stride):
                seq = data[i:i + window_size]
                sequences.append(seq)
            
            return np.array(sequences)
        
        def build_graph_from_features(features, k=5):
            n_samples = len(features)
            
            # Handle edge cases
            if n_samples == 0:
                return np.array([]).reshape(0, 0)
            elif n_samples == 1:
                # Single node - self-loop or empty graph
                return np.ones((1, 1))  # Self-loop for single node
            else:
                # Ensure k is valid
                k = max(1, min(k, n_samples - 1))
                
                # Build adjacency matrix using k-NN
                adj_matrix = kneighbors_graph(features, n_neighbors=k, mode='connectivity', include_self=False)
                adj_matrix = adj_matrix.toarray()
                
                # Make it symmetric (undirected graph)
                adj_matrix = np.maximum(adj_matrix, adj_matrix.T)
                
                return adj_matrix
        
        # DataLoaderM class from old preprocessing for compatibility
        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):
                self.batch_size = batch_size
                self.current_ind = 0
                
                # Ensure float32 for all arrays
                xs = xs.astype(np.float32) if isinstance(xs, np.ndarray) and len(xs) > 0 else xs
                ys = ys.astype(np.float32) if isinstance(ys, np.ndarray) and len(ys) > 0 else ys
                
                # Handle case where ys is None
                if ys is None:
                    ys = xs  # Auto-regressive
                
                if pad_with_last_sample and isinstance(xs, np.ndarray) and len(xs) > 0:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    if num_padding > 0:
                        x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                        y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                        xs = np.concatenate([xs, x_padding], axis=0)
                        ys = np.concatenate([ys, y_padding], axis=0)
                
                self.size = len(xs) if isinstance(xs, np.ndarray) else 0
                self.num_batch = int(self.size // self.batch_size) if self.size > 0 else 0
                self.xs = xs
                self.ys = ys
        
            def shuffle(self):
                if self.size > 0:
                    permutation = np.random.permutation(self.size)
                    self.xs = self.xs[permutation]
                    self.ys = self.ys[permutation]
        
            def get_iterator(self):
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        x_i = self.xs[start_ind: end_ind, ...]
                        y_i = self.ys[start_ind: end_ind, ...]
                        yield (x_i, y_i)
                        self.current_ind += 1
                return _wrapper()
        
        def preprocess_for_gnn(train_df, test_df, anomaly_df, config):
            print(f"Input shapes - Train: {train_df.shape}, Test: {test_df.shape}, Anomaly: {anomaly_df.shape}")
            
            # Drop is_anomaly column if present
            for df_name, df in [('train', train_df), ('test', test_df), ('anomaly', anomaly_df)]:
                if 'is_anomaly' in df.columns:
                    df = df.drop(columns=['is_anomaly'])
                    if df_name == 'train':
                        train_df = df
                    elif df_name == 'test':
                        test_df = df
                    else:
                        anomaly_df = df
            
            # Verify feast_entity_id is already dropped
            if 'feast_entity_id' in train_df.columns:
                train_df = train_df.drop(columns=['feast_entity_id'])
                test_df = test_df.drop(columns=['feast_entity_id'])
                anomaly_df = anomaly_df.drop(columns=['feast_entity_id'])
            
            # Expected columns
            expected_cols = [
                'ratio_cpu', 'ratio_memory', 'ratio_storage', 'ratio_pods',
                'cond_ready', 'cond_memorypressure', 'cond_diskpressure', 
                'cond_pidpressure', 'cond_networkunavailable',
                'closeness', 'clustering'
            ]
            
            # Use available columns
            available_cols = [col for col in expected_cols if col in train_df.columns]
            print(f"Available columns: {available_cols}")
            
            train_df = train_df[available_cols].copy()
            test_df = test_df[available_cols].copy()
            anomaly_df = anomaly_df[available_cols].copy()
            
            # Convert all to numeric (using .loc to avoid SettingWithCopyWarning)
            for df in [train_df, test_df, anomaly_df]:
                for col in df.columns:
                    df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
            
            # Fill NaN values
            train_df = train_df.fillna(train_df.median())
            test_df = test_df.fillna(test_df.median())
            anomaly_df = anomaly_df.fillna(anomaly_df.median())
            
            # Convert to numpy arrays WITH EXPLICIT FLOAT32
            train_data = train_df.values.astype(np.float32)
            test_data = test_df.values.astype(np.float32)
            anomaly_data = anomaly_df.values.astype(np.float32)
            
            print(f"Data shapes after cleaning:")
            print(f"  Train: {train_data.shape}")
            print(f"  Test: {test_data.shape}")
            print(f"  Anomaly: {anomaly_data.shape}")
            
            # ========== WINDOW CREATION ==========
            window_size = config['window_size']
            
            # Create windows with stride = window_size // 2 (50% overlap)
            stride = max(1, window_size // 2)
            train_windows = create_sliding_windows(train_data, window_size, stride)
            print(f"Created {len(train_windows)} training windows (stride={stride})")
            
            # Create evaluation windows
            if len(anomaly_data) > 0:
                eval_data = np.vstack([test_data, anomaly_data])
            else:
                eval_data = test_data
                print("Warning: No anomaly data available")
            
            eval_windows = create_sliding_windows(eval_data, window_size, stride)
            print(f"Created {len(eval_windows)} evaluation windows")
            
            # ========== NODE FEATURES AND SCALING ==========
            train_node_features = train_windows.mean(axis=1)
            eval_node_features = eval_windows.mean(axis=1) if len(eval_windows) > 0 else np.array([])
            
            scaler = StandardScaler()
            train_node_features_scaled = scaler.fit_transform(train_node_features).astype(np.float32)
            eval_node_features_scaled = scaler.transform(eval_node_features).astype(np.float32) if len(eval_node_features) > 0 else np.array([]).astype(np.float32)
            
            # ========== GRAPH BUILDING ==========
            print("Building graphs...")
            train_adj_matrix = build_graph_from_features(train_node_features_scaled, k=config['k_neighbors'])
            train_adj_matrix = train_adj_matrix.astype(np.float32) if train_adj_matrix.size > 0 else train_adj_matrix
            
            # Split evaluation data
            val_ratio = config['val_ratio']
            if len(eval_node_features_scaled) > 1:
                val_size = max(1, int(len(eval_node_features_scaled) * val_ratio))
                val_node_features = eval_node_features_scaled[:val_size]
                test_node_features = eval_node_features_scaled[val_size:]
                
                # Ensure we have at least 1 test sample
                if len(test_node_features) == 0 and len(val_node_features) > 1:
                    # Move one from validation to test
                    test_node_features = val_node_features[-1:]
                    val_node_features = val_node_features[:-1]
            elif len(eval_node_features_scaled) == 1:
                # Only one evaluation sample - use for test
                val_node_features = np.array([]).astype(np.float32)
                test_node_features = eval_node_features_scaled
            else:
                val_node_features = np.array([]).astype(np.float32)
                test_node_features = np.array([]).astype(np.float32)
            
            # Build validation and test graphs
            val_adj_matrix = build_graph_from_features(val_node_features, k=min(config['k_neighbors'], max(1, len(val_node_features)-1)))
            val_adj_matrix = val_adj_matrix.astype(np.float32) if val_adj_matrix.size > 0 else val_adj_matrix
            
            test_adj_matrix = build_graph_from_features(test_node_features, k=min(config['k_neighbors'], max(1, len(test_node_features)-1)))
            test_adj_matrix = test_adj_matrix.astype(np.float32) if test_adj_matrix.size > 0 else test_adj_matrix
            
            print(f"Graph sizes - Train: {len(train_node_features_scaled)}, Val: {len(val_node_features)}, Test: {len(test_node_features)}")
            
            # ========== LABELS ==========
            # Create labels (0=normal, 1=anomaly)
            if len(anomaly_data) > 0:
                # Calculate how many windows come from anomaly data
                test_normal_windows = len(create_sliding_windows(test_data, window_size, stride))
                anomaly_windows = len(create_sliding_windows(anomaly_data, window_size, stride))
                
                # Create labels for all evaluation windows
                total_eval_windows = test_normal_windows + anomaly_windows
                eval_labels = np.zeros(total_eval_windows, dtype=np.int32)
                if anomaly_windows > 0:
                    eval_labels[test_normal_windows:] = 1  # Mark anomaly windows
            else:
                # No anomalies - all windows are normal
                eval_labels = np.zeros(len(eval_windows), dtype=np.int32)
            
            # Split labels to match validation/test split
            if len(eval_labels) > 0:
                val_labels = eval_labels[:len(val_node_features)] if len(val_node_features) > 0 else np.array([], dtype=np.int32)
                test_labels = eval_labels[len(val_node_features):] if len(test_node_features) > 0 else np.array([], dtype=np.int32)
            else:
                val_labels = np.array([], dtype=np.int32)
                test_labels = np.array([], dtype=np.int32)
            
            # ========== CREATE STGNN-COMPATIBLE DATA STRUCTURE ==========
            # STGNN expects: [samples, 100, 11, 1]
            
            batch_size = config.get('batch_size', 64)
            
            def create_stgnn_data(features):
                if len(features) == 0:
                    return np.array([], dtype=np.float32)
                
                samples = len(features)
                num_features = features.shape[1]
                
                # Create array with correct shape
                x_3d = np.zeros((samples, 100, 11, 1), dtype=np.float32)
                
                # Fill data: repeat features across time dimension
                for i in range(samples):
                    for t in range(100):  # All time steps get same value
                        for f in range(min(num_features, 11)):  # Up to 11 nodes
                            x_3d[i, t, f, 0] = features[i, f]
                
                return x_3d
            
            if len(train_node_features_scaled) > 0:
                x_train_3d = create_stgnn_data(train_node_features_scaled)
                y_train_3d = np.zeros((len(train_node_features_scaled), 1, 11, 1), dtype=np.float32)  # seq_out_len=1
                # Fill y with same data (auto-regressive)
                for i in range(len(train_node_features_scaled)):
                    for f in range(min(train_node_features_scaled.shape[1], 11)):
                        y_train_3d[i, 0, f, 0] = train_node_features_scaled[i, f]
            else:
                x_train_3d = np.array([], dtype=np.float32)
                y_train_3d = np.array([], dtype=np.float32)
            
            if len(val_node_features) > 0:
                x_val_3d = create_stgnn_data(val_node_features)
                y_val_3d = np.zeros((len(val_node_features), 1, 11, 1), dtype=np.float32)
                for i in range(len(val_node_features)):
                    for f in range(min(val_node_features.shape[1], 11)):
                        y_val_3d[i, 0, f, 0] = val_node_features[i, f]
            else:
                x_val_3d = np.array([], dtype=np.float32)
                y_val_3d = np.array([], dtype=np.float32)
            
            if len(test_node_features) > 0:
                x_test_3d = create_stgnn_data(test_node_features)
                y_test_3d = np.zeros((len(test_node_features), 1, 11, 1), dtype=np.float32)
                for i in range(len(test_node_features)):
                    for f in range(min(test_node_features.shape[1], 11)):
                        y_test_3d[i, 0, f, 0] = test_node_features[i, f]
            else:
                x_test_3d = np.array([], dtype=np.float32)
                y_test_3d = np.array([], dtype=np.float32)
            
            # Create DataLoaderM objects for compatibility
            train_loader = DataLoaderM(x_train_3d, y_train_3d, batch_size) if len(x_train_3d) > 0 else None
            val_loader = DataLoaderM(x_val_3d, y_val_3d, batch_size) if len(x_val_3d) > 0 else None
            test_loader = DataLoaderM(x_test_3d, y_test_3d, batch_size) if len(x_test_3d) > 0 else None
            
            print(f"STGNN shapes - Train: {x_train_3d.shape}, Val: {x_val_3d.shape}, Test: {x_test_3d.shape}")
            
            # ========== METADATA ==========
            metadata = {
                'num_nodes_train': len(train_node_features_scaled),
                'num_nodes_val': len(val_node_features),
                'num_nodes_test': len(test_node_features),
                'num_features': train_data.shape[1],
                'feature_names': available_cols,
                'window_size': window_size,
                'stride': stride,
                'k_neighbors': config['k_neighbors'],
                'train_anomaly_ratio': 0.0,
                'val_anomaly_ratio': val_labels.mean() if len(val_labels) > 0 else 0,
                'test_anomaly_ratio': test_labels.mean() if len(test_labels) > 0 else 0,
                'adj_matrix_density': {
                    'train': float(train_adj_matrix.mean()) if train_adj_matrix.size > 0 else 0,
                    'val': float(val_adj_matrix.mean()) if val_adj_matrix.size > 0 else 0,
                    'test': float(test_adj_matrix.mean()) if test_adj_matrix.size > 0 else 0
                }
            }
            
            # ========== DATA WRAPPER WITH COMPATIBILITY ATTRIBUTES ==========
            data_dict = {
                # New v3 attributes
                'x_train': train_node_features_scaled,
                'x_val': val_node_features,
                'x_test': test_node_features,
                'adj_train': train_adj_matrix,
                'adj_val': val_adj_matrix,
                'adj_test': test_adj_matrix,
                'y_train': np.zeros(len(train_node_features_scaled), dtype=np.int32),
                'y_val': val_labels,
                'y_test': test_labels,
                'train_windows': train_windows.astype(np.float32) if len(train_windows) > 0 else train_windows,
                'val_windows': eval_windows[:len(val_node_features)].astype(np.float32) if len(val_node_features) > 0 else np.array([], dtype=np.float32),
                'test_windows': eval_windows[len(val_node_features):].astype(np.float32) if len(test_node_features) > 0 else np.array([], dtype=np.float32),
                'scaler': scaler,
                'metadata': metadata,
                
                # REQUIRED COMPATIBILITY ATTRIBUTES (from old preprocessing)
                'train_mask': np.ones(len(train_node_features_scaled), dtype=bool) if len(train_node_features_scaled) > 0 else np.array([], dtype=bool),
                'test_mask': np.ones(len(test_node_features), dtype=bool) if len(test_node_features) > 0 else np.array([], dtype=bool),
                'num_nodes': len(train_node_features_scaled),
                'num_features': train_data.shape[1],
                'feature_dim': train_data.shape[1],
                
                # DataLoader objects for compatibility
                'train_loader': train_loader,
                'val_loader': val_loader,
                'test_loader': test_loader,
                
                # STGNN-compatible data
                'x_train_3d': x_train_3d,
                'x_val_3d': x_val_3d,
                'x_test_3d': x_test_3d,
                'y_train_3d': y_train_3d,
                'y_val_3d': y_val_3d,
                'y_test_3d': y_test_3d,
                
                # Add batch_size for reference
                'batch_size': batch_size,
            }
            
            # ========== FINAL FIX: ENSURE ALL NUMPY ARRAYS ARE FLOAT32 ==========
            def ensure_float32_dict(d):
                for key, value in d.items():
                    if isinstance(value, np.ndarray) and value.dtype != np.float32 and np.issubdtype(value.dtype, np.floating):
                        d[key] = value.astype(np.float32)
                return d
            
            data_dict = ensure_float32_dict(data_dict)
            
            return DataWrapper(data_dict), metadata
        
        if __name__ == '__main__':
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_path', type=str, required=True)
            parser.add_argument('--test_path', type=str, required=True)
            parser.add_argument('--anomaly_path', type=str, required=True)
            parser.add_argument('--window_size', type=int, default=10)
            parser.add_argument('--output_pickle', type=str, required=True)
            parser.add_argument('--output_metadata', type=str, required=True)
            args = parser.parse_args()
            
            print("Starting GNN preprocessing for Kubernetes metrics...")
            
            # Load datasets
            train_df = pd.read_parquet(args.train_path)
            test_df = pd.read_parquet(args.test_path)
            anomaly_df = pd.read_parquet(args.anomaly_path)
            
            print(f"Loaded datasets:")
            print(f"  Train: {train_df.shape[0]} samples")
            print(f"  Test: {test_df.shape[0]} samples")
            print(f"  Anomaly: {anomaly_df.shape[0]} samples")
            
            # Configuration with batch_size for compatibility
            config = {
                'window_size': args.window_size,
                'k_neighbors': min(3, max(1, train_df.shape[0] // 30)),  # Adaptive k
                'val_ratio': 0.3,
                'batch_size': 64  # Same as model config
            }
            
            print(f"Configuration: {config}")
            
            # Preprocess for GNN
            data_wrapper, metadata = preprocess_for_gnn(train_df, test_df, anomaly_df, config)
            
            # Save processed data
            os.makedirs(os.path.dirname(args.output_pickle) or '.', exist_ok=True)
            with open(args.output_pickle, 'wb') as f:
                pickle.dump(data_wrapper, f)
            
            # Save metadata
            os.makedirs(os.path.dirname(args.output_metadata) or '.', exist_ok=True)
            with open(args.output_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"\\nPreprocessing complete!")
            print(f"Dataset statistics:")
            print(f"  - Training nodes: {metadata['num_nodes_train']}")
            print(f"  - Validation nodes: {metadata['num_nodes_val']}")
            print(f"  - Test nodes: {metadata['num_nodes_test']}")
            print(f"  - Features: {metadata['num_features']}")
            print(f"  - STGNN shape: {data_wrapper.x_train_3d.shape if hasattr(data_wrapper, 'x_train_3d') else 'Not found'}")
            print(f"  - Added train_mask: {data_wrapper.train_mask.shape if hasattr(data_wrapper, 'train_mask') else 'Not found'}")
            print(f"  - Data types check: All float32 arrays verified")
        EOF
        
        # Run the preprocessing script
        python preprocess_gnn.py \
          --train_path "$0" \
          --test_path "$1" \
          --anomaly_path "$2" \
          --window_size "$3" \
          --output_pickle "$4" \
          --output_metadata "$5"
        
    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: anomaly_data}
      - {inputValue: window_size}
      - {outputPath: processed_data_pickle}
      - {outputPath: metadata_json}
