name: Preprocess PPI Data
description: Analyzes class distribution in the PPI dataset and outputs alpha & class weights as JSON.
inputs:
  - name: PPI_data
    type: Dataset
outputs:
  - name: weights_out1
    type: String
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch torch_geometric || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch torch_geometric --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import torch
        import json

        def analyze_class_distribution(train_dataset):
            """
            Compute per-class distribution for multi-label PPI dataset.
            """
            # Concatenate all node labels across graphs in the training split
            all_labels = torch.cat([graph.y for graph in train_dataset], dim=0)  # shape [num_nodes, num_classes]

            # Count how many nodes have each class
            class_counts = all_labels.sum(dim=0)  # [num_classes]
            total_nodes = all_labels.shape[0]

            print(f"[INFO] Total nodes in train split: {total_nodes}")
            print(f"[INFO] Number of classes: {all_labels.shape[1]}")

            # --- Class Weights (for BCEWithLogitsLoss) ---
            # Inverse frequency weighting: larger weight for rare classes
            class_weights = (total_nodes / (len(class_counts) * (class_counts + 1e-6))).tolist()

            # --- Alpha Weights (for Focal Loss) ---
            # Proportional to class frequency
            alpha_weights = (class_counts / total_nodes).tolist()

            return alpha_weights, class_weights

        parser = argparse.ArgumentParser()
        parser.add_argument('--PPI_data', type=str, required=True)
        parser.add_argument('--weights_out1', type=str, required=True)
        args = parser.parse_args()

        print(f"[INFO] Input dataset path: {args.PPI_data}")
        print(f"[INFO] Output JSON path: {args.weights_out1}")

        # Load dataset dict
        try:
            with open(args.PPI_data, "rb") as f:
                data_dict = pickle.load(f)
            print(f"[INFO] Successfully loaded dataset dict with keys: {list(data_dict.keys())}")
        except Exception as e:
            print(f"[ERROR] Failed to load dataset: {e}")
            import traceback; traceback.print_exc()
            exit(1)

        # Use only train split for distribution analysis
        train_dataset = data_dict["train"]

        # Ensure output directory exists
        os.makedirs(os.path.dirname(args.weights_out1), exist_ok=True)

        # Analyze class distribution
        try:
            alpha_weights, class_weights = analyze_class_distribution(train_dataset)

            weights_dict = {
                'focal_loss_alpha': alpha_weights,
                'class_weights': class_weights
            }

            # Save JSON
            with open(args.weights_out1, 'w') as f:
                json.dump(weights_dict, f, indent=2)

            print(f"[SUCCESS] Saved weights to {args.weights_out1}")
            print(f"[INFO] Example values (first 5 classes):")
            print(f"  Class Weights: {class_weights[:5]}")
            print(f"  Alpha Weights: {alpha_weights[:5]}")

        except Exception as e:
            print(f"[ERROR] Failed during analysis: {e}")
            import traceback; traceback.print_exc()
            exit(1)
    args:
      - --PPI_data
      - {inputPath: PPI_data}
      - --weights_out1
      - {outputPath: weights_out1}
