name: Load and extract text from structured document API
description: Fetches nested JSON from API and extracts all content and titles for model training
inputs:
- name: api_url
  type: String
- name: access_token
  type: String
outputs:
- name: extracted_text
  type: Data
- name: metadata
  type: Data
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command: [python3, -c]
    args:
    - |
      import argparse
      import os
      import json
      import pandas as pd
      import requests
      from requests.adapters import HTTPAdapter
      from urllib3.util.retry import Retry
      import logging
      import re
      
      parser = argparse.ArgumentParser()
      parser.add_argument('--api_url', type=str, required=True)
      parser.add_argument('--access_token', type=str, required=True)
      parser.add_argument('--extracted_text', type=str, required=True)
      parser.add_argument('--metadata', type=str, required=True)
      args = parser.parse_args()
      
      logging.basicConfig(level=logging.INFO)
      logger = logging.getLogger('extractor')
      
      token_arg = args.access_token.strip()
      if os.path.exists(token_arg):
          with open(token_arg) as f:
              access_token = f.read().strip()
      else:
          access_token = token_arg
      
      session = requests.Session()
      retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
      adapter = HTTPAdapter(max_retries=retries)
      session.mount('http://', adapter)
      session.mount('https://', adapter)
      
      headers = {'Authorization': 'Bearer ' + access_token}
      
      try:
          resp = session.get(args.api_url, headers=headers, timeout=60)
          resp.raise_for_status()
          raw_data = resp.json()
      except Exception as e:
          logger.error('API failed: ' + str(e))
          raise
      
      CHILD_KEYS = ['subsections', 'sections', 'children', 'items']
      TITLE_KEYS = ['title', 'heading', 'name']
      CONTENT_KEYS = ['content', 'text', 'body']
      
      def iter_children(node):
          for k in CHILD_KEYS:
              v = node.get(k)
              if isinstance(v, list):
                  return v
          return []
      
      def get_first(node, keys):
          for k in keys:
              val = node.get(k)
              if val:
                  return str(val).strip()
          return ''
      
      def extract_recursive(node, path=''):
          results = []
          title = get_first(node, TITLE_KEYS)
          content = get_first(node, CONTENT_KEYS)
          if title or content:
              results.append({'title': title, 'content': content, 'path': path})
          children = iter_children(node)
          for idx, child in enumerate(children):
              subpath = path + '/sub_' + str(idx) if path else 'sub_' + str(idx)
              results.extend(extract_recursive(child, subpath))
          return results
      
      all_rows = []
      response_items = raw_data if isinstance(raw_data, list) else [raw_data]
      
      for resp_idx, response_data in enumerate(response_items):
          response_obj = response_data.get('response', {}) if response_data else {}
          individual_results = response_obj.get('individualResults', [])
      
          for ind_idx, result in enumerate(individual_results):
              original_result = result.get('originalResult', {})
              results_list = original_result.get('results', [])
      
              for res_idx, res in enumerate(results_list):
                  data = res.get('data', {})
                  structured_data = data.get('structuredTextJsonData', {})
                  document = structured_data.get('document', {})
                  
                  path_prefix = 'r' + str(resp_idx) + '_i' + str(ind_idx) + '_s' + str(res_idx)
                  extracted_items = extract_recursive(document, path_prefix)
                  
                  for item in extracted_items:
                      item['response_index'] = resp_idx
                      item['individual_index'] = ind_idx
                      item['url'] = result.get('url', '')
                  
                  all_rows.extend(extracted_items)
      
      logger.info('Extracted ' + str(len(all_rows)) + ' segments')
      
      if all_rows:
          df = pd.DataFrame(all_rows)
      else:
          df = pd.DataFrame(columns=['title', 'content'])
      
      def clean_text(s):
          if not s:
              return ''
          s = re.sub(r'\s+', ' ', s)
          return s.strip()
      
      def combine(row):
          t = clean_text(row.get('title', ''))
          c = clean_text(row.get('content', ''))
          parts = []
          if t:
              parts.append(t)
          if c:
              parts.append(c)
          return ' '.join(parts)
      
      if len(df) > 0:
          df['full_text'] = df.apply(combine, axis=1)
          df = df[df['full_text'].str.len() > 0].reset_index(drop=True)
      else:
          df = pd.DataFrame({'full_text': []})
      
      os.makedirs(os.path.dirname(args.extracted_text) or '.', exist_ok=True)
      with open(args.extracted_text, 'w', encoding='utf-8') as f:
          for line in df['full_text']:
              f.write(line)
              f.write('\n')
      
      meta = {}
      
      os.makedirs(os.path.dirname(args.metadata) or '.', exist_ok=True)
      with open(args.metadata, 'w') as f:
          json.dump(meta, f, indent=2)
      
      logger.info('Done')
    - --api_url
    - {inputValue: api_url}
    - --access_token
    - {inputValue: access_token}
    - --extracted_text
    - {outputPath: extracted_text}
    - --metadata
    - {outputPath: metadata}
